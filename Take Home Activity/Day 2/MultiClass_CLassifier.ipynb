{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author : Adeniyi Adeboye \n",
    "\n",
    "- In this assignment, you must create a multi-class classifier using the provided training set and produce class predictions for the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Multiclass Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Major Steps Ahead:\n",
    "\n",
    "- Loading in Data and Data Exploration\n",
    "- Data preprocessing\n",
    "- Dimensionality Reduction using Principal Component Analysis (PCA)\n",
    "\n",
    "### Implementation of Oversampling Method and Machine Learning Algorithm:\n",
    "\n",
    "### 1st Approach towards solving imbalanced problem in our data\n",
    "- Oversampling of the imbalanced dataset using Borderline-SMOTE\n",
    "- Grid Search (Hyperparameter Tuning for RandomForestClassifier based on the SMOTE data)\n",
    "- Cross validation\n",
    "### 2nd Approach \n",
    "- Using Cost-Sensitive RandomForestClassifier based on class_weights\n",
    "- Grid Search and Cross Validation (Here input data is NOT SMOTE data)\n",
    "### 3rd Approach\n",
    "- Oversampling of the imbalanced dataset using Borderline-SMOTE\n",
    "- Using Linear Discriminant Analysis (LDA) as the classifier\n",
    "- Cross validation\n",
    "\n",
    "\n",
    "- Predictions on Test Data\n",
    "- Saving the predictions in a csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold,cross_val_score, cross_validate, cross_val_predict, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the data visualization parameters\n",
    "\n",
    "import matplotlib.pylab as pylab\n",
    "params = {'legend.fontsize': 'large',\n",
    "          'figure.figsize': (14, 6),\n",
    "         'axes.labelsize': 'large',\n",
    "         'axes.titlesize':'large',\n",
    "         'xtick.labelsize':'large',\n",
    "         'ytick.labelsize':'large'}\n",
    "pylab.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in data\n",
    "df_train = pd.read_csv('train_features.csv', names = ['feat_' + str(x) for x in range(1,904)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feat_1      140\n",
       "feat_2      140\n",
       "feat_3      140\n",
       "feat_4      140\n",
       "feat_5      140\n",
       "           ... \n",
       "feat_899    140\n",
       "feat_900    140\n",
       "feat_901    140\n",
       "feat_902    140\n",
       "feat_903    140\n",
       "Length: 903, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count number of rows for each column\n",
    "df_train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>feat_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_894</th>\n",
       "      <th>feat_895</th>\n",
       "      <th>feat_896</th>\n",
       "      <th>feat_897</th>\n",
       "      <th>feat_898</th>\n",
       "      <th>feat_899</th>\n",
       "      <th>feat_900</th>\n",
       "      <th>feat_901</th>\n",
       "      <th>feat_902</th>\n",
       "      <th>feat_903</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>right</td>\n",
       "      <td>type_3</td>\n",
       "      <td>green</td>\n",
       "      <td>0.8827</td>\n",
       "      <td>3.2917</td>\n",
       "      <td>1.4651</td>\n",
       "      <td>-4.5278</td>\n",
       "      <td>1.0490</td>\n",
       "      <td>2.3398</td>\n",
       "      <td>1.8729</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8854</td>\n",
       "      <td>-0.8752</td>\n",
       "      <td>0.2662</td>\n",
       "      <td>4.9686</td>\n",
       "      <td>2.5588</td>\n",
       "      <td>-0.6121</td>\n",
       "      <td>-3.3692</td>\n",
       "      <td>0.0550</td>\n",
       "      <td>-1.5296</td>\n",
       "      <td>-5.3041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>left</td>\n",
       "      <td>type_2</td>\n",
       "      <td>blue</td>\n",
       "      <td>1.4458</td>\n",
       "      <td>-2.9575</td>\n",
       "      <td>-1.2341</td>\n",
       "      <td>-3.9684</td>\n",
       "      <td>-2.7645</td>\n",
       "      <td>5.6346</td>\n",
       "      <td>1.7838</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.3774</td>\n",
       "      <td>-2.2512</td>\n",
       "      <td>-1.6331</td>\n",
       "      <td>7.2724</td>\n",
       "      <td>1.7616</td>\n",
       "      <td>4.2826</td>\n",
       "      <td>5.5557</td>\n",
       "      <td>1.0588</td>\n",
       "      <td>2.6734</td>\n",
       "      <td>-4.5224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>right</td>\n",
       "      <td>type_2</td>\n",
       "      <td>green</td>\n",
       "      <td>-10.2982</td>\n",
       "      <td>-0.3714</td>\n",
       "      <td>-0.9886</td>\n",
       "      <td>-3.2219</td>\n",
       "      <td>4.0925</td>\n",
       "      <td>-0.8319</td>\n",
       "      <td>-3.0588</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.4408</td>\n",
       "      <td>-2.5580</td>\n",
       "      <td>-1.2116</td>\n",
       "      <td>5.1098</td>\n",
       "      <td>-0.6747</td>\n",
       "      <td>-1.2528</td>\n",
       "      <td>-2.2944</td>\n",
       "      <td>-3.4745</td>\n",
       "      <td>2.8633</td>\n",
       "      <td>1.6737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>right</td>\n",
       "      <td>type_1</td>\n",
       "      <td>red</td>\n",
       "      <td>-8.4566</td>\n",
       "      <td>-0.2408</td>\n",
       "      <td>-3.0342</td>\n",
       "      <td>2.9534</td>\n",
       "      <td>2.8977</td>\n",
       "      <td>0.8851</td>\n",
       "      <td>3.0113</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9434</td>\n",
       "      <td>-0.8771</td>\n",
       "      <td>0.4143</td>\n",
       "      <td>4.3368</td>\n",
       "      <td>-11.2949</td>\n",
       "      <td>-7.4289</td>\n",
       "      <td>7.9900</td>\n",
       "      <td>-6.2433</td>\n",
       "      <td>1.6592</td>\n",
       "      <td>-4.8601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>left</td>\n",
       "      <td>type_1</td>\n",
       "      <td>blue</td>\n",
       "      <td>4.2681</td>\n",
       "      <td>-2.2052</td>\n",
       "      <td>-5.9093</td>\n",
       "      <td>0.1036</td>\n",
       "      <td>1.8462</td>\n",
       "      <td>1.9801</td>\n",
       "      <td>2.1129</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.2290</td>\n",
       "      <td>0.5390</td>\n",
       "      <td>0.7648</td>\n",
       "      <td>1.1348</td>\n",
       "      <td>1.9829</td>\n",
       "      <td>3.7682</td>\n",
       "      <td>-1.7092</td>\n",
       "      <td>1.4791</td>\n",
       "      <td>5.7732</td>\n",
       "      <td>-3.9106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 903 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  feat_1  feat_2 feat_3   feat_4  feat_5  feat_6  feat_7  feat_8  feat_9  \\\n",
       "0  right  type_3  green   0.8827  3.2917  1.4651 -4.5278  1.0490  2.3398   \n",
       "1   left  type_2   blue   1.4458 -2.9575 -1.2341 -3.9684 -2.7645  5.6346   \n",
       "2  right  type_2  green -10.2982 -0.3714 -0.9886 -3.2219  4.0925 -0.8319   \n",
       "3  right  type_1    red  -8.4566 -0.2408 -3.0342  2.9534  2.8977  0.8851   \n",
       "4   left  type_1   blue   4.2681 -2.2052 -5.9093  0.1036  1.8462  1.9801   \n",
       "\n",
       "   feat_10  ...  feat_894  feat_895  feat_896  feat_897  feat_898  feat_899  \\\n",
       "0   1.8729  ...    0.8854   -0.8752    0.2662    4.9686    2.5588   -0.6121   \n",
       "1   1.7838  ...   -4.3774   -2.2512   -1.6331    7.2724    1.7616    4.2826   \n",
       "2  -3.0588  ...   -1.4408   -2.5580   -1.2116    5.1098   -0.6747   -1.2528   \n",
       "3   3.0113  ...    0.9434   -0.8771    0.4143    4.3368  -11.2949   -7.4289   \n",
       "4   2.1129  ...   -0.2290    0.5390    0.7648    1.1348    1.9829    3.7682   \n",
       "\n",
       "   feat_900  feat_901  feat_902  feat_903  \n",
       "0   -3.3692    0.0550   -1.5296   -5.3041  \n",
       "1    5.5557    1.0588    2.6734   -4.5224  \n",
       "2   -2.2944   -3.4745    2.8633    1.6737  \n",
       "3    7.9900   -6.2433    1.6592   -4.8601  \n",
       "4   -1.7092    1.4791    5.7732   -3.9106  \n",
       "\n",
       "[5 rows x 903 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check df\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>feat_1</th>\n",
       "      <td>140</td>\n",
       "      <td>2</td>\n",
       "      <td>left</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_2</th>\n",
       "      <td>140</td>\n",
       "      <td>3</td>\n",
       "      <td>type_3</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_3</th>\n",
       "      <td>140</td>\n",
       "      <td>3</td>\n",
       "      <td>red</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       count unique     top freq\n",
       "feat_1   140      2    left   79\n",
       "feat_2   140      3  type_3   64\n",
       "feat_3   140      3     red   53"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets get the descriptive statistics for the categorical columns\n",
    "\n",
    "df_train.describe(exclude = [np.number]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>feat_4</th>\n",
       "      <td>140.0</td>\n",
       "      <td>-3.706226</td>\n",
       "      <td>4.064704</td>\n",
       "      <td>-12.7532</td>\n",
       "      <td>-6.393700</td>\n",
       "      <td>-4.02240</td>\n",
       "      <td>-0.860025</td>\n",
       "      <td>7.8625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_5</th>\n",
       "      <td>140.0</td>\n",
       "      <td>0.620541</td>\n",
       "      <td>3.014747</td>\n",
       "      <td>-8.0209</td>\n",
       "      <td>-1.665500</td>\n",
       "      <td>0.55955</td>\n",
       "      <td>2.784550</td>\n",
       "      <td>9.0779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_6</th>\n",
       "      <td>140.0</td>\n",
       "      <td>-0.691226</td>\n",
       "      <td>3.295262</td>\n",
       "      <td>-9.6888</td>\n",
       "      <td>-2.625975</td>\n",
       "      <td>-0.65115</td>\n",
       "      <td>1.432475</td>\n",
       "      <td>7.7298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_7</th>\n",
       "      <td>140.0</td>\n",
       "      <td>-2.883438</td>\n",
       "      <td>3.331956</td>\n",
       "      <td>-10.5401</td>\n",
       "      <td>-5.215000</td>\n",
       "      <td>-3.02205</td>\n",
       "      <td>-0.441100</td>\n",
       "      <td>5.2301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_8</th>\n",
       "      <td>140.0</td>\n",
       "      <td>0.798445</td>\n",
       "      <td>3.484214</td>\n",
       "      <td>-8.4934</td>\n",
       "      <td>-1.934025</td>\n",
       "      <td>1.00560</td>\n",
       "      <td>3.353100</td>\n",
       "      <td>10.8560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_899</th>\n",
       "      <td>140.0</td>\n",
       "      <td>0.931779</td>\n",
       "      <td>3.301253</td>\n",
       "      <td>-7.6957</td>\n",
       "      <td>-0.960575</td>\n",
       "      <td>1.21010</td>\n",
       "      <td>3.121575</td>\n",
       "      <td>12.6390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_900</th>\n",
       "      <td>140.0</td>\n",
       "      <td>0.791181</td>\n",
       "      <td>3.131848</td>\n",
       "      <td>-7.0905</td>\n",
       "      <td>-1.299400</td>\n",
       "      <td>0.78565</td>\n",
       "      <td>2.618375</td>\n",
       "      <td>9.7130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_901</th>\n",
       "      <td>140.0</td>\n",
       "      <td>-0.458641</td>\n",
       "      <td>2.616963</td>\n",
       "      <td>-6.8516</td>\n",
       "      <td>-2.491375</td>\n",
       "      <td>-0.05590</td>\n",
       "      <td>1.136900</td>\n",
       "      <td>5.0199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_902</th>\n",
       "      <td>140.0</td>\n",
       "      <td>0.723416</td>\n",
       "      <td>3.382829</td>\n",
       "      <td>-6.1844</td>\n",
       "      <td>-1.769575</td>\n",
       "      <td>0.35025</td>\n",
       "      <td>3.384000</td>\n",
       "      <td>8.4399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_903</th>\n",
       "      <td>140.0</td>\n",
       "      <td>-0.132218</td>\n",
       "      <td>3.657816</td>\n",
       "      <td>-9.7235</td>\n",
       "      <td>-2.728575</td>\n",
       "      <td>0.20005</td>\n",
       "      <td>2.175950</td>\n",
       "      <td>9.6323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          count      mean       std      min       25%      50%       75%  \\\n",
       "feat_4    140.0 -3.706226  4.064704 -12.7532 -6.393700 -4.02240 -0.860025   \n",
       "feat_5    140.0  0.620541  3.014747  -8.0209 -1.665500  0.55955  2.784550   \n",
       "feat_6    140.0 -0.691226  3.295262  -9.6888 -2.625975 -0.65115  1.432475   \n",
       "feat_7    140.0 -2.883438  3.331956 -10.5401 -5.215000 -3.02205 -0.441100   \n",
       "feat_8    140.0  0.798445  3.484214  -8.4934 -1.934025  1.00560  3.353100   \n",
       "...         ...       ...       ...      ...       ...      ...       ...   \n",
       "feat_899  140.0  0.931779  3.301253  -7.6957 -0.960575  1.21010  3.121575   \n",
       "feat_900  140.0  0.791181  3.131848  -7.0905 -1.299400  0.78565  2.618375   \n",
       "feat_901  140.0 -0.458641  2.616963  -6.8516 -2.491375 -0.05590  1.136900   \n",
       "feat_902  140.0  0.723416  3.382829  -6.1844 -1.769575  0.35025  3.384000   \n",
       "feat_903  140.0 -0.132218  3.657816  -9.7235 -2.728575  0.20005  2.175950   \n",
       "\n",
       "              max  \n",
       "feat_4     7.8625  \n",
       "feat_5     9.0779  \n",
       "feat_6     7.7298  \n",
       "feat_7     5.2301  \n",
       "feat_8    10.8560  \n",
       "...           ...  \n",
       "feat_899  12.6390  \n",
       "feat_900   9.7130  \n",
       "feat_901   5.0199  \n",
       "feat_902   8.4399  \n",
       "feat_903   9.6323  \n",
       "\n",
       "[900 rows x 8 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets generate descriptive statistics of the train dataset\n",
    "#for the numeric columns\n",
    "\n",
    "df_train.describe(exclude = [np.object]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feat_1      0\n",
       "feat_2      0\n",
       "feat_3      0\n",
       "feat_4      0\n",
       "feat_5      0\n",
       "           ..\n",
       "feat_899    0\n",
       "feat_900    0\n",
       "feat_901    0\n",
       "feat_902    0\n",
       "feat_903    0\n",
       "Length: 903, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for null values\n",
    "\n",
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bring in the label csv data\n",
    "df_labels = pd.read_csv('train_labels.csv', names = ['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concantenate the label and train data\n",
    "df_trn = pd.concat([df_train, df_labels[['label']]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group-by the class labels\n",
    "\n",
    "label_ct = df_trn.groupby('label', as_index = False).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_894</th>\n",
       "      <th>feat_895</th>\n",
       "      <th>feat_896</th>\n",
       "      <th>feat_897</th>\n",
       "      <th>feat_898</th>\n",
       "      <th>feat_899</th>\n",
       "      <th>feat_900</th>\n",
       "      <th>feat_901</th>\n",
       "      <th>feat_902</th>\n",
       "      <th>feat_903</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>...</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>...</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 904 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  feat_1  feat_2  feat_3  feat_4  feat_5  feat_6  feat_7  feat_8  \\\n",
       "0      0      17      17      17      17      17      17      17      17   \n",
       "1      1      82      82      82      82      82      82      82      82   \n",
       "2      2      41      41      41      41      41      41      41      41   \n",
       "\n",
       "   feat_9  ...  feat_894  feat_895  feat_896  feat_897  feat_898  feat_899  \\\n",
       "0      17  ...        17        17        17        17        17        17   \n",
       "1      82  ...        82        82        82        82        82        82   \n",
       "2      41  ...        41        41        41        41        41        41   \n",
       "\n",
       "   feat_900  feat_901  feat_902  feat_903  \n",
       "0        17        17        17        17  \n",
       "1        82        82        82        82  \n",
       "2        41        41        41        41  \n",
       "\n",
       "[3 rows x 904 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the class name and the value count showing the amount of each class\n",
    "label = list(label_ct['label'])\n",
    "lab_count = list(label_ct['feat_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#unique classes in our data\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17, 82, 41]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#each class count\n",
    "lab_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0EAAAGJCAYAAABM2TgpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3dfZhcZX3/8feyS9CQSKCmaADlSb4VAcEIWqwKP6tF9KcgClioYsqTikhbBKlFRRQUUWuLECwSisGqFQT5VdH+FKGIBQFBAvRbIQFEfOApVAgSdjP940zsdNmHEzJzZjf3+3Vde2XOPWfOfDZezPjJfc59BlqtFpIkSZJUivX6HUCSJEmSmmQJkiRJklQUS5AkSZKkoliCJEmSJBXFEiRJkiSpKJYgSZIkSUUZ6ncASdLUFBGDwHuBP6X6vpgBXAp8MDMfj4jzgCWZeXpDeZ4J3JeZAxHxBuCPM/PoCfZ/HfCSzPzgGM/97vUR8X3gjMz82hpk2Qj4emb+n/b2jcAembl8zX4rSVI/WIIkSeM5C9gYeFVmPhwRGwIXAOcAf9bPYJn5DeAbk+y2K7DJWrx+IhsDu3Ucb+e1OJYkqWGWIEnSk0TElsBBwLMz878AMvPRiDgSeNkY+y8AjqCaLdoE+HhmnhURzwLOB57Z3vVfMvPE8cbHOO6bgI8BK4AfdYwfArw5M1/f3udvgFXACPA+4HHgSGAwIh4Gfgr8ObAh8DDwj6tf3z7kvhHxfmAmcEFmfqz9d7AkM2d1/J2s3l4EPL09AzQfGAbmZub9EXEi8Nb22H8CR2XmL9szTj9s//09B/j/wOGZuWr8/yUkSb3gNUGSpLHMB25ZXYBWy8xfZuaFnWMRMQs4DNg7M3cBDgBOaz99GLA0M18EvBx4XvtUsvHGO4+7KXAusF9mzgfuGifrJ4F3ZeaLgROpTku7BlgIfCUzP9De7wXt5/Yc4xjPAF7a/jk4Il470V8O8A7gsczcOTNHOjK/A3gtsGtm7gQsAc7reN02wB7ATu39XjnJ+0iSesASJEkayypqfkdk5iPA64HXRcTJwAeAWe2nLwP2i4hvUs0UvT8zH55gvNMfATdn5q3t7bPHifBl4OsRcQ7VaWqnjbPfT0aXug7nZOZw+/mvAa+e4FeeyGuBRZn5aHv7s8CrImJGe/vSzFzVfp/bGed0PUlSb1mCJEljuQZ4fkTM7hyMiM0i4l8i4ukdY5sDNwLPBa6iOjUNgMz8EbAV8HlgS+DaiJg/3vgYOQY6Hg+PFbQ90/NHwHXAIcCV4/xOj4wzDtVpdKutBzwBtEa9/wwmN9h+XeexhjqO81jHc6OPL0lqiCVIkvQkmXkv1SII50bEMwDaf54JPJCZnf9n/sXAfcBHge9QzQoREYMR8XHgxMy8mGqluVuAHcYbHxXjSuAFEfHC9vYho3NGxFBE3AnMzMyFwLuAnSJiA6rStH7NX/ltETEQERsD+1PNVC0HZkTE9u193tqx/zDV9UajS8xlwIL2IhIARwNXZubjNXNIkhpgCZIkjeddwK3A1e0FAK5pbx86ar/vAPcACdxGddH/fcC2wN8CO0fEEqqZmmVUp6+NN/47mXkf1fLcF0TEDVQzR4zaZxg4BvhSe59/Bha0S8f3gD+JiL+v8bs+DFwPXA38fWZe3j497zjgWxHxI/73LM4vgGuBWyLi9zrGv0C14MG1EXEb8CKqBSYkSVPIQKvVmnwvSZIkSVpHOBMkSZIkqSiWIEmSJElFsQRJkiRJKoolSJIkSVJRLEGSJEmSijLU7wBP1apVq1ojI65sJ0mSJGls668/eD8wd/T4tC1BIyMtli9f0e8YkiRJkqaouXNn3zXWuKfDSZIkSSqKJUiSJElSUSxBkiRJkopiCZIkSZJUFEuQJEmSpKJYgiRJkiQVxRIkSZIkqSiWIEmSJElFsQRJkiRJKoolSJIkSVJRLEGSJEmSimIJkiRJklSUoX4HkKTpYOHCM1i69PZ+x5Cmta233pYjjzyq3zEkyRIkSXUsXXo7N998U79jSJKkLrAESdIamDnUYquNWv2OIU0ryx4eYMXwQL9jSNLvWIIkaQ1stVGLk1/2RL9jSNPKiT9Yn1sesARJmjpcGEGSJElSURqdCYqIfYGTgFXAg8BhwJ3Ap4C92nlOz8yFTeaSJEmSVI7GZoIi4unAYuBNmbkzcCnwd8ARwHbADsCuwDERsVtTuSRJkiSVpcnT4QaBAWCj9vYs4LfAvsCizBzOzIeALwMHN5hLkiRJUkEaK0GZ+QhwJHB1RNwLHAUcD2wB/Kxj13uAzZvKJUmSJKksjV0TFBE7Ah8Ets/MOyLiaOBCqhmizvVmB4CRyY43ODjAnDkze5JVkkYbGhrsdwRp2hsaGvS7W9KU0OTCCH8C/CAz72hvfw74DHA5MK9jv3lUs0ETGhlpsXz5iq6HlKSxDA9P+m8zkiYxPDzid7ekRs2dO3vM8SavCboBeGVEbNre3gdYBlwCLIiIoYiYAxwIXNxgLkmSJEkFaWwmKDO/FxGfBL4fESuplsh+I5DANsBNwAzg7My8oqlckiRJksrS6H2CMvNzVKfBjXZMkzkkSZIklavJ0+EkSZIkqe8sQZIkSZKKYgmSJEmSVBRLkCRJkqSiWIIkSZIkFcUSJEmSJKkoliBJkiRJRbEESZIkSSqKJUiSJElSUSxBkiRJkopiCZIkSZJUFEuQJEmSpKJYgiRJkiQVxRIkSZIkqSiWIEmSJElFsQRJkiRJKoolSJIkSVJRLEGSJEmSimIJkiRJklQUS5AkSZKkoliCJEmSJBXFEiRJkiSpKJYgSZIkSUWxBEmSJEkqiiVIkiRJUlEsQZIkSZKKYgmSJEmSVBRLkCRJkqSiWIIkSZIkFWWoqTeKiLcBf9kxtBGwefvnBGCvdp7TM3NhU7kkSZIklaWxEpSZ5wPnA0TE+sCVwMeB/YDtgB2A2cAPI+KGzLy2qWySJEmSytGv0+GOB36dmWcD+wKLMnM4Mx8Cvgwc3KdckiRJktZxjc0ErRYRzwT+CpjfHtoC+FnHLvcAOzWdS5IkSVIZGi9BwOHAJZm5tL29HtDqeH4AGJnsIIODA8yZM7MH8STpyYaGBvsdQZr2hoYG/e6WNCX0owQdABzdsX03MK9jex7VbNCERkZaLF++osvRJGlsw8OT/tuMpEkMD4/43S2pUXPnzh5zvNESFBEbA9sCV3cMXwIsiIhLgVnAgcCRTeaSJEmSVI6mZ4K2BX6RmU90jJ0FbAPcBMwAzs7MKxrOJUmSJKkQjZagzPwRVRHqHBsGjmkyhyRJkqRy9WuJbEmSJEnqC0uQJEmSpKJYgiRJkiQVxRIkSZIkqSiWIEmSJElFsQRJkiRJKoolSJIkSVJRLEGSJEmSimIJkiRJklQUS5AkSZKkoliCJEmSJBXFEiRJkiSpKJYgSZIkSUWxBEmSJEkqiiVIkiRJUlEsQZIkSZKKYgmSJEmSVBRLkCRJkqSiWIIkSZIkFcUSJEmSJKkoliBJkiRJRbEESZIkSSqKJUiSJElSUSxBkiRJkopiCZIkSZJUFEuQJEmSpKJYgiRJkiQVxRIkSZIkqSiWIEmSJElFGWryzSJiR+DvgY2AEeCIzLw+Ik4A3t7Osxg4KTNbTWaTJEmSVIbGZoIiYibwHeC0zNwFOBm4ICL2BvYH5gM7AHsCb2kqlyRJkqSyNHk63GuAOzLzm+3tb1CVn32BL2Xmo5n5W2ARcHCDuSRJkiQVpMnT4bYDfhkRXwBeCCwHjgO2AL7bsd89wOYN5pIkSZJUkCZL0PrA3sCemXlNRLwR+CZwG9B5/c8A1fVCExocHGDOnJk9CSpJow0NDfY7gjTtDQ0N+t0taUposgTdC9yWmdcAZOYlEXEOsAqY17HfPKrZoAmNjLRYvnxFT4JK0mjDw5P+24ykSQwPj/jdLalRc+fOHnO8yWuCvgVsFRHzASLiFVQzQH8LHBQRG0bEBsAhwMUN5pIkSZJUkMZKUGb+EtgHODMilgCfAd6UmZcCFwHXAkuA64Hzm8olSZIkqSyN3icoM68EXjLG+CnAKU1mkSRJklSmJk+HkyRJkqS+swRJkiRJKoolSJIkSVJRLEGSJEmSimIJkiRJklQUS5AkSZKkoliCJEmSJBXFEiRJkiSpKJYgSZIkSUUZqrtjROwE7AgMtocGgA2AXTPzsB5kkyRJkqSuq1WCIuJY4DRgFVX5aVHNIrWAy3uWTpIkSZK6rO7pcO8GPkI183Mf8Bzg+cDNwLd6E02SJEmSuq9uCdoMOD8zR4AbgZdkZgJ/Bfx5r8JJkiRJUrfVLUEPA09rP/4psEPH4+d2O5QkSZIk9UrdEvR94NSIeDZwLfDmiNgIeAPwQI+ySZIkSVLX1S1B7wO2Ag4EvkK1QMKDwGfbP5IkSZI0LdRaHS4z7wR2ioinZebKiHgZsAdwX2b+qIf5JEmSJKmras0ERcTSiNgkM38LkJkrMvObwD0R8eueJpQkSZKkLhp3Jigi9gZe3N7cEnh/RDwyarftJjqGJEmSJE01ExWYZcDfUt0cFeDNwEjH8y3gN8B7ehNNkiRJkrpv3BKUmbdRzfQQEZcDb8rMh5oKJkmSJEm9UHdhhD3HGo+IGcCumfmDrqaSJEmSpB6pVYIiYj7wD8COjL2YwmA3Q0mSJElSr9S9T9BngceAw4GVwDuB04DHgQN6E02SJEmSuq9uCdoFODozFwE3ApmZJwDHUxUiSZIkSZoW6pagAeC+9uOfUp0WB3Ap8MJuh5IkSZKkXqlbgpYAe7cf3wq8rP14U7weSJIkSdI0UvdGp58AvhIRI8A/AR+KiIupZoEu71U4SZIkSeq2WjNBmXkh8FLg2sy8i2pWaCXwTeCw3sWTJEmSpO6qOxNEZl7X8fhynsIMUER8CngL8OD/HCoPiIgTgLe38ywGTsrM1poeX5IkSZImM24Jiohz6x4kMxfU3HV34MDMvLrjffYG9gfmAyPAt6muO/pq3feXJEmSpLommgnaouPxILAH8HPgBqpT4XYBngtcVOeNImKD9muOi4htgP8E/gLYF/hSZj7a3m8RcDCWIEmSJEk9MG4JysxXr37cPo3tLuDwzHyiPTYAnAFsWPO95gHfA/4GuAU4FrgE+DXw3Y797gE2n+xgg4MDzJkzs+ZbS9LaGRpyIUxpbQ0NDfrdLWlKqHtN0KHAbqsLEEBmtiLis1QzQ4dMdoDMXMb/LLNNRJwOnAjcD3Re/zNAdVrchEZGWixfvqJmfElaO8PDk34sSZrE8PCI392SGjV37uwxx+veJ+hRYPsxxncDHqhzgIjYKSL+bNTwANUM07yOsXlUs0GSJEmS1HV1Z4L+AfhCRDyfauZngGqRg/cCH6x5jFXA30XEVe1ZoXcCP6E6Je5DEfF5YJhqVum8ur+AJEmSJK2JuiXow1QF5T3Apu2xnwMnZuZn6xwgM5dExHuASyNikGq2562ZeXdE7AhcC8ygKkXn1/8VJEmSJKm+WiWofc+ek4GTI+KZQCsza50GN+o4i6nuAzR6/BTglDU9niRJkiStqdo3S10tM+/vRRBJkiRJakLdhREkSZIkaZ1gCZIkSZJUlHFLUEScFhEbtx8/p31zVEmSJEma1iaaCXoPsFH78TLgmb2PI0mSJEm9NdHCCMuAr0fEjVT3Bfq7iHhsrB0zc0EvwkmSJElSt01Ugg4CTgA2A1rAPGBlE6EkSZIkqVfGLUGZ+WNgf4CIWAa86ancG0iSJEmSppK6N0vdKiIGIuK1wA7AE8AtwPcyc6SXASVJkiSpm2qVoIjYBPhXYBfgfmAQ2Bj4cUS8OjMf7F1ESZIkSeqeuvcJ+gxV8dk+M38/M3+PakZoAPhEr8JJkiRJUrfVLUGvB96dmf+xeiAzbwWOBt7Yi2CSJEmS1At1S9AA8NAY4w8CG3YvjiRJkiT1Vt0S9EPg+IgYXD3Qfvx+4JpeBJMkSZKkXqi1MAJwPHAVcHtEXNse2w3YCHh1L4JJkiRJUi/UmgnKzCXAC4F/BmZRlacvAn+Qmdf3Lp4kSZIkdVfdmSAy8y7guB5mkSRJkqSeq3tNkCRJkiStEyxBkiRJkopiCZIkSZJUlFolKCLOi4jteh1GkiRJknqt7kzQPsDKXgaRJEmSpCbULUEXAB+JiG0jovaKcpIkSZI01dQtNK8CtgMOAloRsarzycyc0e1gkiRJktQLdUvQqT1NIUmSJEkNqVWCMvMfex1EkiRJkppQ+/qeiHgFcALwB8AewDuAOzLzi72JJkmSJEndV6sERcRrgQuBxVQFaBBoAedGxFBmLupZQkmSpClk4cIzWLr09n7HkKa9rbfeliOPPKov7113JujDwLGZeWZEvBUgM0+KiIeAYwFLkCRJKsLSpbdz88039TuGpLVQtwS9ALhsjPFLgU+syRtGxD7AFzNzdnv7BODt7SyLgZMys7Umx5QkSWrc+jCwyUC/U0jTTuvBFjzR3wx1S9D9wNbA0lHjLwZ+VffNIuJ5wOnAQHt7b2B/YD4wAnwbuBX4at1jSpIk9cPAJgPM2Mu7hEhrauVlK2n9qr9zHnVvlvp54HPta4MGgG0iYgFwBnBenQNExEyqmZ6/7BjeF/hSZj6amb+lOq3u4JqZJEmSJGmNrcl9gjYCLgI2oJqxeQL4NPCRmsc4u/3zk46xLYDvdmzfA2xe52CDgwPMmTOz5ltL0toZGhrsdwRp2hsaGlwnvrv9PJC6o5+fCXXvE9QCjo+IjwDPB1YCP83Mx+q8PiLeBQxn5rkRsWXHU+tRrTK32gDVaXGTGhlpsXz5ijq7StJaGx6u9dEkaQLDwyPrxHe3nwdSdzTxmTB37uwxx9fkPkFPBw4AdgAeB5ZExFcyc7jGyw8BZkbEjcAM4OntxzcA8zr2m0c1GyRJkiRJPVHrmqCI2I5qUYTPAC8D/hhYCNwSEVtP9vrM3C0zd8jMnYG9gcfaj78OHBQRG0bEBlRl6eKn9JtIkiRJUg11F0Y4B7gK2CwzX5KZuwLPpSpGn3uqb56Zl1JdZ3QtsAS4Hjj/qR5PkiRJkiZT93S4XYEXZeYjqwcy88GIOA64Zk3eMDPvBGZ1bJ8CnLImx5AkSZKkp6ruTNBSYNsxxjfDa3gkSZIkTSPjzgRFxO4dm4uBL0TEB4AfUq3gtgvwSeovkS1JkiRJfTfR6XBXUS1fPdAxdvYY+505zrgkSZIkTTkTlaCtGkshSZIkSQ0ZtwRl5l1NBpEkSZKkJtRaHS4itqJawW0HYIPRz2fmdl3OJUmSJEk9UXeJ7POpVoL7KvBY7+JIkiRJUm/VLUEvAl6emTf0MowkSZIk9Vrd+wT9FJjZyyCSJEmS1IS6M0FHAWdExKepbpy6qvPJzLy628EkSZIkqRfqlqA/AJ4PnDfGcy1gsFuBJEmSJKmX6pagk4AvAGcAj/YujiRJkiT1Vt0StBFwWmbe2cMskiRJktRzdRdG+BqwTy+DSJIkSVIT6s4E3QmcEhH7AbcDT3Q+mZmHdzmXJEmSJPVE3RL0SuCa9uMtexNFkiRJknqvVgnKzD17HUSSJEmSmlCrBEXE7hM9732CJEmSJE0XdU+Hu4rqfkADHWOt9s8qYEaXc0mSJElST9QtQVuN8brtgI8Cx3c1kSRJkiT1UN1rgu4aY/iOiPgNcBawY1dTSZIkSVKP1L1P0Hh+DWzbjSCSJEmS1IS1WRjhGcBfAEu6mkiSJEmSemhtFkaA6iaqB3czkCRJkiT10lNdGAFgZWb+opthJEmSJKnX1mZhBEmSJEmadsYtQRHx+ZrHaGXmEV3KI0mSJEk9NdFM0PMmee3WwBbAE4AlSJIkSdK0MG4Jysw9xxqPiCHgA8DuwI3Agt5EkyRJkqTuq7swAgARsQuwCAjgZODUzBxZg9cfBbyTaqW5O4DDgAeATwF7tfOcnpkL1ySXJEmSJNVV9z5BM4APA+8DrgfmZ+ata/JGETEfOBZ4YWY+HBGnUxWpm4DtgB2A2cAPI+KGzLx2TY4vSZIkSXWsN9kOEfFS4MfAe4ETgN3XtAABZOb1wPPaBehpwGZUs0D7AosyczgzHwK+jPcekiRJktQjE60O9zTgFOA9wNXAGzLzjrV5s8x8IiL2Ac4BHgc+CLwJ+FnHbvcAO012rMHBAebMmbk2cSSptqGhwX5HkKa9oaHBdeK7288DqTv6+Zkw0elwPwG2AZYC3wEOiIgxd8zMU+q+YWZeDFwcEYcB3waGqa4RWm0AmPQ6o5GRFsuXr6j7tpK0VoaHa1/+KGkcw8Mj68R3t58HUnc08Zkwd+7sMccnKkEzgLvb+xw6wX4tqhmjCUXEtsCzMvOq9tC5wELgSmBex67zqGaDJEmSJKnrJloie8suv9ezgX+KiJ0z837gIGAJcBGwICIuBWYBBwJHdvm9JUmSJAlYwyWy10Zm/ltEfAz4fkQMA/cC+1BdD7QN1SpxM4CzM/OKpnJJkiRJKktjJQggM88CzhrjqWOazCFJkiSpXJMukS1JkiRJ6xJLkCRJkqSiWIIkSZIkFcUSJEmSJKkoliBJkiRJRbEESZIkSSqKJUiSJElSUSxBkiRJkopiCZIkSZJUFEuQJEmSpKJYgiRJkiQVxRIkSZIkqSiWIEmSJElFsQRJkiRJKoolSJIkSVJRLEGSJEmSimIJkiRJklQUS5AkSZKkoliCJEmSJBXFEiRJkiSpKJYgSZIkSUWxBEmSJEkqiiVIkiRJUlEsQZIkSZKKYgmSJEmSVBRLkCRJkqSiWIIkSZIkFcUSJEmSJKkoliBJkiRJRRlq8s0i4mDgfUALWAEcnZnXRcQJwNvbeRYDJ2Vmq8lskiRJksrQ2ExQRATwSWCvzNwZ+ChwUUTsDewPzAd2APYE3tJULkmSJEllafJ0uMeBQzPzF+3t64BnURWeL2Xmo5n5W2ARcHCDuSRJkiQVpLHT4TLzTuBOgIgYAD4NfAN4NvDtjl3vATaf7HiDgwPMmTOz6zklaSxDQ4P9jiBNe0NDg+vEd7efB1J39PMzodFrggAiYkPgPGALYC/gq1TXCK02AIxMdpyRkRbLl6/oRURJepLh4Uk/liRNYnh4ZJ347vbzQOqOJj4T5s6dPeZ4o6vDRcRzgKupSs6embkcuBuY17HbPKrZIEmSJEnqusZmgiJiNvB94B8z86SOpy4BPhQRnweGgUOoZookSZIkqeuaPB3uKOC5wL4RsW/H+KuAi4BrgRlUpej8BnNJkiRJKkiTCyOcCpw6ztOntH8kSZIkqacavSZIkiRJkvrNEiRJkiSpKJYgSZIkSUWxBEmSJEkqSuM3S51OFi48g6VLb+93DGla23rrbTnyyKP6HUOSJOl3LEETWLr0dm6++aZ+x5AkSZLURZagGlpDM2DW3H7HkKaXR+5jYHhlv1NIkiQ9iSWojllzYZf9+p1Cml5+fCEs/3m/U0iSJD2JCyNIkiRJKoolSJIkSVJRLEGSJEmSimIJkiRJklQUS5AkSZKkoliCJEmSJBXFEiRJkiSpKJYgSZIkSUWxBEmSJEkqiiVIkiRJUlEsQZIkSZKKYgmSJEmSVBRLkCRJkqSiWIIkSZIkFcUSJEmSJKkoliBJkiRJRbEESZIkSSqKJUiSJElSUSxBkiRJkopiCZIkSZJUlKGm3zAiBoDzgJsz8/SIGAQ+BezVznN6Zi5sOpckSZKkMjQ6ExQRzwe+C7y5Y/gIYDtgB2BX4JiI2K3JXJIkSZLK0fTpcO8GzgH+uWNsX2BRZg5n5kPAl4GDG84lSZIkqRCNng6XmUcBRMRrOoa3AH7WsX0PsFOTuSRJkiSVo/FrgsawHtDq2B4ARiZ70eDgAHPmzOxZKIChocGeHl8qwdDQYM//W22CnwfS2vPzQFKnfn4mTIUSdDcwr2N7HtVs0IRGRlosX76iZ6EAhocn7WKSJjE8PNLz/1ab4OeBtPb8PJDUqYnPhLlzZ485PhVK0CXAgoi4FJgFHAgc2d9IkiRJktZVU6EEnQVsA9wEzADOzswr+htJkiRJ0rqqLyUoMw/peDwMHNOPHJIkSZLK0/QS2ZIkSZLUV5YgSZIkSUWxBEmSJEkqiiVIkiRJUlEsQZIkSZKKYgmSJEmSVBRLkCRJkqSiWIIkSZIkFcUSJEmSJKkoliBJkiRJRbEESZIkSSqKJUiSJElSUSxBkiRJkopiCZIkSZJUFEuQJEmSpKJYgiRJkiQVxRIkSZIkqSiWIEmSJElFsQRJkiRJKoolSJIkSVJRLEGSJEmSimIJkiRJklQUS5AkSZKkoliCJEmSJBXFEiRJkiSpKJYgSZIkSUWxBEmSJEkqiiVIkiRJUlEsQZIkSZKKMtTvAAAR8TrgVGAD4CfAn2fmf/U3lSRJkqR1Ud9ngiJiLrAI2C8zA1gKfLy/qSRJkiStq/pegoDXAD/KzJ+2t88CDoqIgT5mkiRJkrSOmgqnw20B/Kxj+x7gGcBsYGqcEvfIffDjC/udQppeHrmv3wl6YtnDA5z4g/X7HUOaVpY9vG7+u2brwRYrL1vZ7xjStNN6sNXvCAy0Wv0NERF/DWyRme9sbw8BTwCzMvPRCV56H3BXAxElSZIkTU/PBeaOHpwKM0F3Ay/p2N4MeGiSAgRj/DKSJEmSNJmpcE3Qd4CXRsTz2ttHApf0MY8kSZKkdVjfT4cDiIi9qZbIngHcAbwtMx/sbypJkiRJ66IpUYIkSZIkqSlT4XQ4SZIkSWqMJUiSJElSUabC6nDSGomIQeCjwCFU95O6DHh3Zv6qn7kk9V9EnA0MZuah/c4iqXkRsSlwGvAa4OnANcBfZeaSvgbTlONMkKajDwNvB94GvALYHPButlLBImIgIj4CHN7vLJL6IyLWA74ObAe8EdgdeBj4bkT8Xj+zaeqxBGlaiYgZwHuBv87Mf83MG4ADgZdFxO79TSepHyJia+B7wDup7j0nqUwvBP4QWJCZ12bmrcCfAbOA1/U1maYcS5Cmm52pToH7/uqBzLwTuBN4eV8SSeq3PwSWAjsCy/qcRVL/3A28HsiOsVXAALBxXxJpyvKaIE03m7f//Pmo8XuBLRrOImkKyMwLgAsAIqLPaST1S2Y+APzLqOGjgacB32k+kaYyZ4I03eIzMl4AAAQ9SURBVMwEVmXmE6PGH6f6kJMkSSIi3gCcCnw6M2/rdx5NLZYgTTePAetFxOhZzA2AR/uQR5IkTTERcQjVoklfAY7rbxpNRZYgTTc/a//57FHj83jyKXKSJKkwEfEBYBGwEHhbZq7qcyRNQV4TpOnmJuA3wCuBxQARsSWwJXBl31JJkqS+i4jjqO4l+MHMPLnfeTR1WYI0rWTm4xFxJnB6RNwP/Bo4E7giM/+9v+kkSVK/RMROwCnAucA/RMSzOp7+TWZ62rx+x9PhNB39DdVKUIuBy4G7gDf3NZEkSeq3A4FBYAHwi1E/f9HHXJqCBlqtVr8zSJIkSVJjnAmSJEmSVBRLkCRJkqSiWIIkSZIkFcUSJEmSJKkoliBJkiRJRbEESZIkSSqKN0uVJE0JETEDOBo4CHge8ChwDfCRzLwuIrYElgEvz8yr+hZUkjTtORMkSeq7iJgJ/BvwTuB0YGdgL+BB4N8iYs8+xpMkrWOcCZIkTQUfBbYDXpCZ93aMHxIRvw+cAby+L8kkSescS5Akqa/ap8G9A/jCqAK02ruB2UBr1Os2oZo1ei3wTOA+4ALg+MxcFRGbAmcBrwSeBvwQODYzb2y//hDgeGBr4JfAecBJmbmqy7+iJGmKsQRJkvpta2AO8O9jPZmZywDa1wR1Op+q/Pxf4AGqMnQG8APgYuBMYH3gj4BVwMeBC4FtImIn4GzgrcB1wHyqAnVH+7iSpHWYJUiS1G8bt/9cvoavuwy4PDNvaW+fGRHHAztSlaBtgZ8AyzLztxFxBLB9RKwHbEM1s3RXZt4N3B0Rfwzcs5a/iyRpGrAESZL67f72n5us4evOAt4YEYdSXU+0E7A5MNh+/mSqWZ39IuIK4FvA+e1T5S6jWnnuuoi4Hfg28JV2IZIkreNcHU6S1G93AL8GXjrWkxGxR0R8A3h2x9gA8E3g08AK4ItU1/7cuXqfzPwaMA84lOqanxOBH0fEppn5WGa+EtiVqijtAlwREcd1/beTJE05A61Wa/K9JEnqoYj4BHAEsH3n4gjtsvMtquuG/gRYCrwceAhYAszPzBva+z6Dqkx9gmoW6OPA4o6FEH4f+BVwQPv1L83Mkzve60xgj8zcvre/rSSp3zwdTpI0FZwMvAa4KiI+QHWq2qbAsVQzPK/mf68O9xAwDOwfEQ9SzRJ9DNgA2CAzhyPiRcDLI+JoqnJ0EPAEcAOwGfChiHgYuBR4FrAn4yzOIElat3g6nCSp7zLzEeAVwJeAD1HN8lxE9T31h5l51aj976VaVvstwH8Ai6mK02KqU9wA/hRYBvw/4DZgH+CNmXl7Zl4BLAAOB26lWkjhCuDo3v2WkqSpwtPhJEmSJBXFmSBJkiRJRbEESZIkSSqKJUiSJElSUSxBkiRJkopiCZIkSZJUFEuQJEmSpKJYgiRJkiQVxRIkSZIkqSiWIEmSJElF+W9NgWEkkHrezQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1008x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot class distribution\n",
    "index = np.arange(len(label))\n",
    "sns.barplot(x=label, y=lab_count, linewidth=2.5, errcolor=\".2\", edgecolor=\".2\")\n",
    "plt.xlabel('Class', fontsize=15)\n",
    "plt.ylabel('Number of data', fontsize=15)\n",
    "plt.xticks(index, label, fontsize=15)\n",
    "plt.title('Class distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clearly, the data set is imbalanced**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Due to the large amount of dimensions (903 columns) we have in the data we will be performing Dimensionality Reduction in the next section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140, 903)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df shape --> (rows, columns)\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['feat_1', 'feat_2', 'feat_3', 'feat_4', 'feat_5', 'feat_6', 'feat_7',\n",
       "       'feat_8', 'feat_9', 'feat_10',\n",
       "       ...\n",
       "       'feat_894', 'feat_895', 'feat_896', 'feat_897', 'feat_898', 'feat_899',\n",
       "       'feat_900', 'feat_901', 'feat_902', 'feat_903'],\n",
       "      dtype='object', length=903)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "- Because our data is a combination of categorical and numeric data, we will first extract the categorical data and perform one-hot encoder on it to make it numeric.\n",
    "\n",
    "\n",
    "- Secondly, we will standardize the entirely data (including the one-hot encoded) to have a mean == 0 and standard deviation == 1, \n",
    "\n",
    "\n",
    "- This is because it is required for all data that is to be passed into Principal Component Analysis (PCA) -- simply because PCA looks for/at variance and if you do not standardize your features, they will have varying weights in the PCA which will affect the amount of variance given to each principal components in the linear subspace.  we can define PCA as the identification of a linear subspace of lower dimensionality where the largest variance in the original dataset is maintained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the categorical columns into a dataframe\n",
    "\n",
    "df_cat = df_train[['feat_1', 'feat_2', 'feat_3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>right</td>\n",
       "      <td>type_3</td>\n",
       "      <td>green</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>left</td>\n",
       "      <td>type_2</td>\n",
       "      <td>blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>right</td>\n",
       "      <td>type_2</td>\n",
       "      <td>green</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>right</td>\n",
       "      <td>type_1</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>left</td>\n",
       "      <td>type_1</td>\n",
       "      <td>blue</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  feat_1  feat_2 feat_3\n",
       "0  right  type_3  green\n",
       "1   left  type_2   blue\n",
       "2  right  type_2  green\n",
       "3  right  type_1    red\n",
       "4   left  type_1   blue"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "left     79\n",
       "right    61\n",
       "Name: feat_1, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cat.feat_1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "left     79\n",
       "right    61\n",
       "Name: feat_1, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cat.feat_1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "left     79\n",
       "right    61\n",
       "Name: feat_1, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cat.feat_1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>columns</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>values</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>blue</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>green</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>left</th>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>red</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type_1</th>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type_2</th>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type_3</th>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "columns  feat_1  feat_2  feat_3\n",
       "values                         \n",
       "blue          0       0      36\n",
       "green         0       0      51\n",
       "left         79       0       0\n",
       "red           0       0      53\n",
       "right        61       0       0\n",
       "type_1        0      44       0\n",
       "type_2        0      32       0\n",
       "type_3        0      64       0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the counts of each value/class in each categorical column\n",
    "\n",
    "df_cats = df_cat.melt(var_name='columns', value_name='values')\n",
    "pd.crosstab(index=df_cats['values'], columns=df_cats['columns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the categorical columns into binary columns\n",
    "\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "df_trans = ohe.fit_transform(df_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 1., 0.],\n",
       "       [1., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 1., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 1., 1., ..., 0., 0., 1.],\n",
       "       [0., 1., 0., ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_trans = make_column_transformer((ohe, ['feat_1', 'feat_2', 'feat_3']), remainder = 'passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform only the categorical columns in the train dataframe into binary columns leaving the numeric columns\n",
    "\n",
    "df_trans = pd.DataFrame(col_trans.fit_transform(df_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>898</th>\n",
       "      <th>899</th>\n",
       "      <th>900</th>\n",
       "      <th>901</th>\n",
       "      <th>902</th>\n",
       "      <th>903</th>\n",
       "      <th>904</th>\n",
       "      <th>905</th>\n",
       "      <th>906</th>\n",
       "      <th>907</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8827</td>\n",
       "      <td>3.2917</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8854</td>\n",
       "      <td>-0.8752</td>\n",
       "      <td>0.2662</td>\n",
       "      <td>4.9686</td>\n",
       "      <td>2.5588</td>\n",
       "      <td>-0.6121</td>\n",
       "      <td>-3.3692</td>\n",
       "      <td>0.0550</td>\n",
       "      <td>-1.5296</td>\n",
       "      <td>-5.3041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.4458</td>\n",
       "      <td>-2.9575</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.3774</td>\n",
       "      <td>-2.2512</td>\n",
       "      <td>-1.6331</td>\n",
       "      <td>7.2724</td>\n",
       "      <td>1.7616</td>\n",
       "      <td>4.2826</td>\n",
       "      <td>5.5557</td>\n",
       "      <td>1.0588</td>\n",
       "      <td>2.6734</td>\n",
       "      <td>-4.5224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-10.2982</td>\n",
       "      <td>-0.3714</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.4408</td>\n",
       "      <td>-2.5580</td>\n",
       "      <td>-1.2116</td>\n",
       "      <td>5.1098</td>\n",
       "      <td>-0.6747</td>\n",
       "      <td>-1.2528</td>\n",
       "      <td>-2.2944</td>\n",
       "      <td>-3.4745</td>\n",
       "      <td>2.8633</td>\n",
       "      <td>1.6737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-8.4566</td>\n",
       "      <td>-0.2408</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9434</td>\n",
       "      <td>-0.8771</td>\n",
       "      <td>0.4143</td>\n",
       "      <td>4.3368</td>\n",
       "      <td>-11.2949</td>\n",
       "      <td>-7.4289</td>\n",
       "      <td>7.9900</td>\n",
       "      <td>-6.2433</td>\n",
       "      <td>1.6592</td>\n",
       "      <td>-4.8601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.2681</td>\n",
       "      <td>-2.2052</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.2290</td>\n",
       "      <td>0.5390</td>\n",
       "      <td>0.7648</td>\n",
       "      <td>1.1348</td>\n",
       "      <td>1.9829</td>\n",
       "      <td>3.7682</td>\n",
       "      <td>-1.7092</td>\n",
       "      <td>1.4791</td>\n",
       "      <td>5.7732</td>\n",
       "      <td>-3.9106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 908 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7        8       9    ...     898  \\\n",
       "0  0.0  1.0  0.0  0.0  1.0  0.0  1.0  0.0   0.8827  3.2917  ...  0.8854   \n",
       "1  1.0  0.0  0.0  1.0  0.0  1.0  0.0  0.0   1.4458 -2.9575  ... -4.3774   \n",
       "2  0.0  1.0  0.0  1.0  0.0  0.0  1.0  0.0 -10.2982 -0.3714  ... -1.4408   \n",
       "3  0.0  1.0  1.0  0.0  0.0  0.0  0.0  1.0  -8.4566 -0.2408  ...  0.9434   \n",
       "4  1.0  0.0  1.0  0.0  0.0  1.0  0.0  0.0   4.2681 -2.2052  ... -0.2290   \n",
       "\n",
       "      899     900     901      902     903     904     905     906     907  \n",
       "0 -0.8752  0.2662  4.9686   2.5588 -0.6121 -3.3692  0.0550 -1.5296 -5.3041  \n",
       "1 -2.2512 -1.6331  7.2724   1.7616  4.2826  5.5557  1.0588  2.6734 -4.5224  \n",
       "2 -2.5580 -1.2116  5.1098  -0.6747 -1.2528 -2.2944 -3.4745  2.8633  1.6737  \n",
       "3 -0.8771  0.4143  4.3368 -11.2949 -7.4289  7.9900 -6.2433  1.6592 -4.8601  \n",
       "4  0.5390  0.7648  1.1348   1.9829  3.7682 -1.7092  1.4791  5.7732 -3.9106  \n",
       "\n",
       "[5 rows x 908 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#notice our dataframe has increased because of the one-hot encoded columns\n",
    "df_trans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets scale\n",
    "train_scale = scaler.fit_transform(df_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>898</th>\n",
       "      <th>899</th>\n",
       "      <th>900</th>\n",
       "      <th>901</th>\n",
       "      <th>902</th>\n",
       "      <th>903</th>\n",
       "      <th>904</th>\n",
       "      <th>905</th>\n",
       "      <th>906</th>\n",
       "      <th>907</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.138017</td>\n",
       "      <td>1.138017</td>\n",
       "      <td>-0.677003</td>\n",
       "      <td>-0.544331</td>\n",
       "      <td>1.089725</td>\n",
       "      <td>-0.588348</td>\n",
       "      <td>1.321022</td>\n",
       "      <td>-0.780510</td>\n",
       "      <td>1.133023</td>\n",
       "      <td>0.889213</td>\n",
       "      <td>...</td>\n",
       "      <td>0.768688</td>\n",
       "      <td>0.215790</td>\n",
       "      <td>0.022132</td>\n",
       "      <td>1.396795</td>\n",
       "      <td>0.966085</td>\n",
       "      <td>-0.469344</td>\n",
       "      <td>-1.333181</td>\n",
       "      <td>0.196978</td>\n",
       "      <td>-0.668407</td>\n",
       "      <td>-1.419003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.878722</td>\n",
       "      <td>-0.878722</td>\n",
       "      <td>-0.677003</td>\n",
       "      <td>1.837117</td>\n",
       "      <td>-0.917663</td>\n",
       "      <td>1.699673</td>\n",
       "      <td>-0.756990</td>\n",
       "      <td>-0.780510</td>\n",
       "      <td>1.272054</td>\n",
       "      <td>-1.191108</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.921276</td>\n",
       "      <td>-0.207458</td>\n",
       "      <td>-0.533652</td>\n",
       "      <td>2.144536</td>\n",
       "      <td>0.735047</td>\n",
       "      <td>1.018660</td>\n",
       "      <td>1.526775</td>\n",
       "      <td>0.581930</td>\n",
       "      <td>0.578506</td>\n",
       "      <td>-1.204529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.138017</td>\n",
       "      <td>1.138017</td>\n",
       "      <td>-0.677003</td>\n",
       "      <td>1.837117</td>\n",
       "      <td>-0.917663</td>\n",
       "      <td>-0.588348</td>\n",
       "      <td>1.321022</td>\n",
       "      <td>-0.780510</td>\n",
       "      <td>-1.627583</td>\n",
       "      <td>-0.330211</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021710</td>\n",
       "      <td>-0.301828</td>\n",
       "      <td>-0.410311</td>\n",
       "      <td>1.442625</td>\n",
       "      <td>0.028977</td>\n",
       "      <td>-0.664119</td>\n",
       "      <td>-0.988765</td>\n",
       "      <td>-1.156565</td>\n",
       "      <td>0.634844</td>\n",
       "      <td>0.495488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.138017</td>\n",
       "      <td>1.138017</td>\n",
       "      <td>1.477098</td>\n",
       "      <td>-0.544331</td>\n",
       "      <td>-0.917663</td>\n",
       "      <td>-0.588348</td>\n",
       "      <td>-0.756990</td>\n",
       "      <td>1.281214</td>\n",
       "      <td>-1.172885</td>\n",
       "      <td>-0.286735</td>\n",
       "      <td>...</td>\n",
       "      <td>0.787313</td>\n",
       "      <td>0.215205</td>\n",
       "      <td>0.065470</td>\n",
       "      <td>1.191733</td>\n",
       "      <td>-3.048889</td>\n",
       "      <td>-2.541672</td>\n",
       "      <td>2.306839</td>\n",
       "      <td>-2.218384</td>\n",
       "      <td>0.277621</td>\n",
       "      <td>-1.297184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.878722</td>\n",
       "      <td>-0.878722</td>\n",
       "      <td>1.477098</td>\n",
       "      <td>-0.544331</td>\n",
       "      <td>-0.917663</td>\n",
       "      <td>1.699673</td>\n",
       "      <td>-0.756990</td>\n",
       "      <td>-0.780510</td>\n",
       "      <td>1.968891</td>\n",
       "      <td>-0.940672</td>\n",
       "      <td>...</td>\n",
       "      <td>0.410837</td>\n",
       "      <td>0.650788</td>\n",
       "      <td>0.168036</td>\n",
       "      <td>0.152465</td>\n",
       "      <td>0.799182</td>\n",
       "      <td>0.862280</td>\n",
       "      <td>-0.801239</td>\n",
       "      <td>0.743113</td>\n",
       "      <td>1.498130</td>\n",
       "      <td>-1.036670</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 908 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0 -1.138017  1.138017 -0.677003 -0.544331  1.089725 -0.588348  1.321022   \n",
       "1  0.878722 -0.878722 -0.677003  1.837117 -0.917663  1.699673 -0.756990   \n",
       "2 -1.138017  1.138017 -0.677003  1.837117 -0.917663 -0.588348  1.321022   \n",
       "3 -1.138017  1.138017  1.477098 -0.544331 -0.917663 -0.588348 -0.756990   \n",
       "4  0.878722 -0.878722  1.477098 -0.544331 -0.917663  1.699673 -0.756990   \n",
       "\n",
       "        7         8         9    ...       898       899       900       901  \\\n",
       "0 -0.780510  1.133023  0.889213  ...  0.768688  0.215790  0.022132  1.396795   \n",
       "1 -0.780510  1.272054 -1.191108  ... -0.921276 -0.207458 -0.533652  2.144536   \n",
       "2 -0.780510 -1.627583 -0.330211  ...  0.021710 -0.301828 -0.410311  1.442625   \n",
       "3  1.281214 -1.172885 -0.286735  ...  0.787313  0.215205  0.065470  1.191733   \n",
       "4 -0.780510  1.968891 -0.940672  ...  0.410837  0.650788  0.168036  0.152465   \n",
       "\n",
       "        902       903       904       905       906       907  \n",
       "0  0.966085 -0.469344 -1.333181  0.196978 -0.668407 -1.419003  \n",
       "1  0.735047  1.018660  1.526775  0.581930  0.578506 -1.204529  \n",
       "2  0.028977 -0.664119 -0.988765 -1.156565  0.634844  0.495488  \n",
       "3 -3.048889 -2.541672  2.306839 -2.218384  0.277621 -1.297184  \n",
       "4  0.799182  0.862280 -0.801239  0.743113  1.498130 -1.036670  \n",
       "\n",
       "[5 rows x 908 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scaled data with the same weights; \n",
    "df_scale = pd.DataFrame(train_scale)\n",
    "df_scale.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of the dataset:  0.0\n",
      "Standard deviation of the dataset:  1.0\n"
     ]
    }
   ],
   "source": [
    "#cross check if the Mean == 0 and Standard Deviation == 1\n",
    "\n",
    "print(\"Mean of the dataset: \", np.mean(train_scale).round(8))\n",
    "print(\"Standard deviation of the dataset: \", np.std(train_scale).round(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction / Dimensionality Reduction using PCA (Principal Component Analysis)\n",
    "\n",
    "- PCA can be defined as the identification of a linear subspace of lower dimensionality where the largest variance in the original dataset is maintained.\n",
    "\n",
    "\n",
    "- **Why did we use PCA:**\n",
    "\n",
    "- First of all, We used PCA in case our features exhibited multicolinearity (two or more features are linearly dependent), this is because the principal components are **orthogonal to (i.e., uncorrelated with) the preceding or succeeding principal components.**\n",
    "\n",
    "\n",
    "- Also, since our data is imbalanced, PCA is the most suited dimensionality reduction method, simply because PCA ignores class labels and focuses on finding the principal components that maximizes the variance in a given data. **Thus it is an unsupervised algorithm.**\n",
    "\n",
    "\n",
    "- I used **100 principal components(PC)** because they were able to explain about 90% of the variance in the data, and also because I tried using **120** which is equivalent to explaining 93% variance but it didnt improve any of the model performance I used in the later sections compared, so I sticked with 100 PCS.\n",
    "\n",
    "\n",
    "- Furthermore, because our training dataset has just 140 instances. we cannot choose past ~ 140 PCs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets extract features with the most variance in our dataset\n",
    "\n",
    "pca = PCA(n_components=100)\n",
    "pc = pca.fit_transform(df_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16.38706527, -6.0589946 ,  8.38416588, ...,  3.08169169,\n",
       "        -1.94370113, -0.57788914],\n",
       "       [15.70248449, -4.5934495 ,  0.09096411, ..., -0.69603686,\n",
       "         1.9374524 ,  3.58250299],\n",
       "       [-8.57623159, -2.22975613,  1.61261285, ..., -2.78323488,\n",
       "         1.40623798,  1.47684115],\n",
       "       ...,\n",
       "       [-6.71834635, -1.4287859 ,  2.27855896, ...,  0.89496324,\n",
       "         2.18786682,  2.59397707],\n",
       "       [-7.65411854,  0.0874528 , -3.57211748, ...,  0.74122594,\n",
       "        -1.26989577,  2.48607177],\n",
       "       [-7.90568588,  0.65867623, -1.22000264, ...,  1.55923274,\n",
       "         2.20965216, -2.95013239]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#principal components\n",
    "pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pc = pd.DataFrame(data = pc, columns = ['pc_' + str(x) for x in range(1,101)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pc_1</th>\n",
       "      <th>pc_2</th>\n",
       "      <th>pc_3</th>\n",
       "      <th>pc_4</th>\n",
       "      <th>pc_5</th>\n",
       "      <th>pc_6</th>\n",
       "      <th>pc_7</th>\n",
       "      <th>pc_8</th>\n",
       "      <th>pc_9</th>\n",
       "      <th>pc_10</th>\n",
       "      <th>...</th>\n",
       "      <th>pc_91</th>\n",
       "      <th>pc_92</th>\n",
       "      <th>pc_93</th>\n",
       "      <th>pc_94</th>\n",
       "      <th>pc_95</th>\n",
       "      <th>pc_96</th>\n",
       "      <th>pc_97</th>\n",
       "      <th>pc_98</th>\n",
       "      <th>pc_99</th>\n",
       "      <th>pc_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.387065</td>\n",
       "      <td>-6.058995</td>\n",
       "      <td>8.384166</td>\n",
       "      <td>2.364974</td>\n",
       "      <td>3.544005</td>\n",
       "      <td>0.617851</td>\n",
       "      <td>1.084719</td>\n",
       "      <td>1.443567</td>\n",
       "      <td>-1.807668</td>\n",
       "      <td>-1.624674</td>\n",
       "      <td>...</td>\n",
       "      <td>0.726050</td>\n",
       "      <td>-1.685098</td>\n",
       "      <td>-0.836544</td>\n",
       "      <td>1.749744</td>\n",
       "      <td>-1.208932</td>\n",
       "      <td>0.983156</td>\n",
       "      <td>-0.175768</td>\n",
       "      <td>3.081692</td>\n",
       "      <td>-1.943701</td>\n",
       "      <td>-0.577889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.702484</td>\n",
       "      <td>-4.593449</td>\n",
       "      <td>0.090964</td>\n",
       "      <td>-1.708392</td>\n",
       "      <td>2.712919</td>\n",
       "      <td>-3.967777</td>\n",
       "      <td>-3.319761</td>\n",
       "      <td>-0.962601</td>\n",
       "      <td>-0.460542</td>\n",
       "      <td>3.069255</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.271421</td>\n",
       "      <td>-1.873110</td>\n",
       "      <td>-0.513771</td>\n",
       "      <td>1.796255</td>\n",
       "      <td>-2.061472</td>\n",
       "      <td>0.108098</td>\n",
       "      <td>-0.468144</td>\n",
       "      <td>-0.696037</td>\n",
       "      <td>1.937452</td>\n",
       "      <td>3.582503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-8.576232</td>\n",
       "      <td>-2.229756</td>\n",
       "      <td>1.612613</td>\n",
       "      <td>7.366799</td>\n",
       "      <td>2.982232</td>\n",
       "      <td>0.918906</td>\n",
       "      <td>-3.055563</td>\n",
       "      <td>-2.813001</td>\n",
       "      <td>-6.033587</td>\n",
       "      <td>2.993969</td>\n",
       "      <td>...</td>\n",
       "      <td>3.562168</td>\n",
       "      <td>-2.696021</td>\n",
       "      <td>2.216735</td>\n",
       "      <td>0.800740</td>\n",
       "      <td>0.888942</td>\n",
       "      <td>1.384064</td>\n",
       "      <td>1.717285</td>\n",
       "      <td>-2.783235</td>\n",
       "      <td>1.406238</td>\n",
       "      <td>1.476841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-6.849984</td>\n",
       "      <td>0.123223</td>\n",
       "      <td>-4.929006</td>\n",
       "      <td>1.999662</td>\n",
       "      <td>-1.846568</td>\n",
       "      <td>1.779721</td>\n",
       "      <td>3.444830</td>\n",
       "      <td>1.899716</td>\n",
       "      <td>-4.471761</td>\n",
       "      <td>-0.622429</td>\n",
       "      <td>...</td>\n",
       "      <td>1.731490</td>\n",
       "      <td>2.900396</td>\n",
       "      <td>-0.798207</td>\n",
       "      <td>4.372258</td>\n",
       "      <td>1.336528</td>\n",
       "      <td>-3.415604</td>\n",
       "      <td>-1.602600</td>\n",
       "      <td>-0.201397</td>\n",
       "      <td>1.657028</td>\n",
       "      <td>-0.444199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15.613714</td>\n",
       "      <td>-4.726705</td>\n",
       "      <td>-2.194103</td>\n",
       "      <td>-0.076362</td>\n",
       "      <td>6.987985</td>\n",
       "      <td>3.969398</td>\n",
       "      <td>-1.122759</td>\n",
       "      <td>2.354287</td>\n",
       "      <td>2.472236</td>\n",
       "      <td>-6.083448</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.593182</td>\n",
       "      <td>0.218364</td>\n",
       "      <td>3.003286</td>\n",
       "      <td>-0.467408</td>\n",
       "      <td>-0.998942</td>\n",
       "      <td>2.529843</td>\n",
       "      <td>-2.583191</td>\n",
       "      <td>2.331626</td>\n",
       "      <td>-1.582116</td>\n",
       "      <td>-2.933576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        pc_1      pc_2      pc_3      pc_4      pc_5      pc_6      pc_7  \\\n",
       "0  16.387065 -6.058995  8.384166  2.364974  3.544005  0.617851  1.084719   \n",
       "1  15.702484 -4.593449  0.090964 -1.708392  2.712919 -3.967777 -3.319761   \n",
       "2  -8.576232 -2.229756  1.612613  7.366799  2.982232  0.918906 -3.055563   \n",
       "3  -6.849984  0.123223 -4.929006  1.999662 -1.846568  1.779721  3.444830   \n",
       "4  15.613714 -4.726705 -2.194103 -0.076362  6.987985  3.969398 -1.122759   \n",
       "\n",
       "       pc_8      pc_9     pc_10  ...     pc_91     pc_92     pc_93     pc_94  \\\n",
       "0  1.443567 -1.807668 -1.624674  ...  0.726050 -1.685098 -0.836544  1.749744   \n",
       "1 -0.962601 -0.460542  3.069255  ... -1.271421 -1.873110 -0.513771  1.796255   \n",
       "2 -2.813001 -6.033587  2.993969  ...  3.562168 -2.696021  2.216735  0.800740   \n",
       "3  1.899716 -4.471761 -0.622429  ...  1.731490  2.900396 -0.798207  4.372258   \n",
       "4  2.354287  2.472236 -6.083448  ... -2.593182  0.218364  3.003286 -0.467408   \n",
       "\n",
       "      pc_95     pc_96     pc_97     pc_98     pc_99    pc_100  \n",
       "0 -1.208932  0.983156 -0.175768  3.081692 -1.943701 -0.577889  \n",
       "1 -2.061472  0.108098 -0.468144 -0.696037  1.937452  3.582503  \n",
       "2  0.888942  1.384064  1.717285 -2.783235  1.406238  1.476841  \n",
       "3  1.336528 -3.415604 -1.602600 -0.201397  1.657028 -0.444199  \n",
       "4 -0.998942  2.529843 -2.583191  2.331626 -1.582116 -2.933576  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now the columns are now the principal components and they represent the amount of variance in the dataset\n",
    "df_pc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12151846 0.04595446 0.0147367  0.01419061 0.01409927 0.01365374\n",
      " 0.01337361 0.01280953 0.01250007 0.01207195 0.01203003 0.01188629\n",
      " 0.0116467  0.0116066  0.01140182 0.01121366 0.01107772 0.01091144\n",
      " 0.01072083 0.01058751 0.01033802 0.01020284 0.01014945 0.00993115\n",
      " 0.00989057 0.0098288  0.00975203 0.00953527 0.00947306 0.00915788\n",
      " 0.00892069 0.00879819 0.00873968 0.00859307 0.00848446 0.00838677\n",
      " 0.00823994 0.00818118 0.00807762 0.00797514 0.00787043 0.00778379\n",
      " 0.00770447 0.00758406 0.00748181 0.00741074 0.00717968 0.00716288\n",
      " 0.00703356 0.00699074 0.00692012 0.00679048 0.00671424 0.00668901\n",
      " 0.00653772 0.00643742 0.00635583 0.00627315 0.00622956 0.0061149\n",
      " 0.00607354 0.00597356 0.00587746 0.0058572  0.00580647 0.00570105\n",
      " 0.00561973 0.00554735 0.00550009 0.00539185 0.00537127 0.00527974\n",
      " 0.00521472 0.00512919 0.00505207 0.0050003  0.0049323  0.00484996\n",
      " 0.0047671  0.00468903 0.00465709 0.00454688 0.00453796 0.00447131\n",
      " 0.00443371 0.0043568  0.00426413 0.00417642 0.00413647 0.00411989\n",
      " 0.00399285 0.00394658 0.00389372 0.00384556 0.00373155 0.00368263\n",
      " 0.00358865 0.00357071 0.00349874 0.00348062]\n"
     ]
    }
   ],
   "source": [
    "#the amount of variance that each principal components account for\n",
    "\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting the Cumulative Summation of the Explained Variance\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance') #for each component\n",
    "plt.title('Dataset Explained Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  **As you observed, we reduced the dimensions from over 900 columns to 100 columns, wherein these 100 columns explain aprroximately 90% of the variance in our data which is very good for our dataset**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \tImplementation of Oversampling Method and Machine Learning Algorithm:\n",
    "\n",
    "\n",
    "## 1st Approach \n",
    "\n",
    "### Oversampling of the Imbalanced dataset\n",
    "\n",
    "### Borderline SMOTE (-Synthetic Minority Oversampling Technique) \n",
    "\n",
    "- SMOTE is an **over-sampling approach** in which the minority class is over-sampled by creating **“synthetic”** examples rather than by over-sampling with replacement.\n",
    "\n",
    "\n",
    "- SMOTE first selects a minority class instance a at random and finds its k nearest minority class neighbors. The synthetic instance is then created by choosing one of the k nearest neighbors b at random and connecting a and b to form a line segment in the feature space. \n",
    "\n",
    "\n",
    "- ***Reasons for choosing Borderline-SMOTE:***\n",
    "\n",
    "- We used Borderline SMOTE because it takes into account minority classes at the borderline between another class that are easily misclassified.\n",
    "\n",
    "\n",
    "- Also, Based on results from [Han et al., 2005](https://sci2s.ugr.es/keel/keel-dataset/pdfs/2005-Han-LNCS.pdf), Borderline SMOTE has **higher True Positive (TP) and F1-Score** when used on various datasets than the normal SMOTE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the training data ready for oversampling and further analysis\n",
    "x_trn = df_pc.values\n",
    "y_trn = df_labels['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class count BEFORE smote:  Counter({1: 82, 2: 41, 0: 17})\n",
      "Class distribution AFTER smote:  Counter({1: 82, 2: 82, 0: 82})\n"
     ]
    }
   ],
   "source": [
    "#lets over sample the data\n",
    "#Note we only oversample the minority classes NOT the majority class\n",
    "counter = Counter(y_trn)\n",
    "print(\"Class count BEFORE smote: \", counter)\n",
    "# transform the dataset\n",
    "oversample = BorderlineSMOTE()\n",
    "x_smote, y_smote = oversample.fit_resample(x_trn, y_trn)\n",
    "#new class distribution\n",
    "counter = Counter(y_smote)\n",
    "print(\"Class distribution AFTER smote: \", counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "246"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As you observed, after oversampling the each class label has equal amount of counts relative to another one**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model \n",
    "\n",
    "- Baseline model - simplest possible model that other optimized models will be compared to. It will serve as our benchmark for this multiclass classification task\n",
    "\n",
    "### GridSearch CV -- Tuning of Hyperparameters to get the best out of our desired model\n",
    "\n",
    "- **Grid Search:**   Grid search is a method that is used to perform hyper-parameter optimisation, that is, it is a method to find the best combination of hyper-parameter. Basically, GridSearch tune multiple combinations of hyperparameters, cross validate each and determine which one gives the best performance.\n",
    "\n",
    "\n",
    "- hyperparameter - a parameter whose value cannot be estimated from the data.\n",
    "\n",
    "\n",
    "- **5-fold cross validation** was used through this section, it split the data into 5 bins, use each (1) bin as testing data and use (4) rest of the data as training data and validate against the testing data. Repeating this process 5 times. And thereafter, getting the average performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Baseline model will be a starting point for us \n",
    "\n",
    "base_model = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **The evaluation metric that we will be focusing on will be** ***accuracy, F1 - Measure, Recall, Precision and Confusion Matrix.*** **In the next sections where we will not be oversampling our dataset, so F1-Measure and Confusion Matrix will be our our Main Focus.**\n",
    "\n",
    "\n",
    "- **The baseline model is going to be the basis for comparison of results with other optmized models. Also, we wont use the oversampled data as input for our baseline model since we want it to be a simple model that tells us that it is highly possible to perform multiclass classification on our data set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Recall:  0.5857142857142856\n",
      "Baseline Precision:  0.37256671899529037\n",
      "Baseline F1:  0.4476328064700158\n",
      "Baseline Accuracy:  0.5857142857142856\n"
     ]
    }
   ],
   "source": [
    "#This will give average evaluation metric value after 5-fold cross validation\n",
    "\n",
    "Baseline_recall = cross_val_score(base_model, x_trn, y_trn, cv=5, scoring='recall_weighted')\n",
    "print('Baseline Recall: ', np.mean(Baseline_recall))\n",
    "Baseline_precision = cross_val_score(base_model, x_trn, y_trn, cv=5, scoring='precision_weighted')\n",
    "print('Baseline Precision: ', np.mean(Baseline_precision))\n",
    "Baseline_f1 = cross_val_score(base_model, x_trn, y_trn, cv=5, scoring='f1_weighted')\n",
    "print('Baseline F1: ', np.mean(Baseline_f1))\n",
    "Baseline_accuracy = cross_val_score(base_model, x_trn, y_trn, cv=5, scoring='accuracy')\n",
    "print('Baseline Accuracy: ', np.mean(Baseline_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This compute average confusion matrix after 5-fold cross validation for each model\n",
    "\n",
    "def avg_conf_matrix(model, X, y):\n",
    "    \n",
    "    conf_arrays = []\n",
    "    \n",
    "    folds = list(StratifiedKFold(n_splits=5, shuffle=True, random_state = 42).split(X, y))\n",
    "    for j, (train_idx, valid_idx) in enumerate(folds):\n",
    "        X_train = X[train_idx]\n",
    "        Y_train = y[train_idx]\n",
    "        X_valid = X[valid_idx]\n",
    "        Y_valid = y[valid_idx]\n",
    "        \n",
    "        model.fit(X_train, Y_train)\n",
    "        valid_pred = model.predict(X_valid)\n",
    "        conf = confusion_matrix(Y_valid, valid_pred)\n",
    "        conf_arrays.append(conf)\n",
    "        \n",
    "    mean_conf_arrays = np.mean(conf_arrays, axis=0).round(0)\n",
    "    print(\"Average confusion matrix after every fold Cross-Validation: \", \"\\n\", mean_conf_arrays)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average confusion matrix after every fold Cross-Validation:  \n",
      " [[ 0.  3.  0.]\n",
      " [ 0. 16.  0.]\n",
      " [ 0.  8.  0.]]\n"
     ]
    }
   ],
   "source": [
    "#base model confusion matrix:\n",
    "#Notice that the model is only predicting on class 1-- this is the effect of imbalanced dataset \n",
    "\n",
    "avg_conf_matrix(base_model, x_trn, y_trn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest Classifier\n",
    "\n",
    "- Random Forest Classifier (RDF) - is an ensemble machine learning algorithm that follows the bagging technique. It is an extension of the bagging estimator algorithm. In RDF, The base estimators in random forest are decision trees. \n",
    "\n",
    "\n",
    "- RDF is said to outperforms other algorithms such as support vector machine, Xgboost, LightGBM, etc., when it comes to imbalanced multiclass dataset especially when optimized appropriately.\n",
    "\n",
    "\n",
    "- [Marques et al., 2017](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5249014/) ***indicated that random forest combined with the SMOTE procedure is well suited for multiclass imbalance datasets based on the utilization of the two for their own specific task.***\n",
    "\n",
    "\n",
    "- After some initial data exploration and machine learning testing, I decided to use RandomForest Classifier as the major machine learning model for this task even though I still utilized LDA (since it is said to be robust for multiclass classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 108 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    8.7s\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:   36.8s\n",
      "[Parallel(n_jobs=-1)]: Done 213 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 384 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 605 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 874 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1193 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1560 tasks      | elapsed:  9.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1620 out of 1620 | elapsed:  9.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=RepeatedStratifiedKFold(n_repeats=3, n_splits=5, random_state=1),\n",
       "             error_score=nan,\n",
       "             estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                              class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              max_samples=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators=100, n_jobs=None,\n",
       "                                              oob_score=False,\n",
       "                                              random_state=None, verbose=0,\n",
       "                                              warm_start=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'max_depth': [6, 8, 10, 20],\n",
       "                         'min_samples_leaf': [4, 8, 12],\n",
       "                         'min_samples_split': [5, 10, 15],\n",
       "                         'n_estimators': [300, 400, 500]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=4)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets start hyperparameter tuning\n",
    "#The model we will be using is Random Forest\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits = 5, n_repeats=3, random_state = 1)\n",
    "\n",
    "rdf = RandomForestClassifier()\n",
    "\n",
    "params = {'max_depth': [6, 8, 10, 20], 'min_samples_split': [5, 10, 15], \n",
    "          'min_samples_leaf' : [4, 8, 12], 'n_estimators' : [300, 400, 500]}\n",
    "\n",
    "grid_clf = GridSearchCV(estimator = rdf, param_grid = params, cv=cv, n_jobs=-1, verbose=4)\n",
    "grid_clf.fit(x_smote, y_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 8, 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 400}\n"
     ]
    }
   ],
   "source": [
    "#best parameters for the model\n",
    "print(grid_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets get the model\n",
    "rdf_smote = grid_clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE_RDF Recall:  0.8663673469387755\n",
      "SMOTE_RDF Precision:  0.8966236728551934\n",
      "SMOTE_RDF F1:  0.8861628390346065\n",
      "SMOTE_RDF Accuracy:  0.8746122448979593\n"
     ]
    }
   ],
   "source": [
    "#lets evaluate the model\n",
    "\n",
    "rdfSmote_recall = cross_val_score(rdf_smote, x_smote, y_smote, cv=5, scoring='recall_weighted')\n",
    "print('SMOTE_RDF Recall: ', np.mean(rdfSmote_recall))\n",
    "rdfSmote_precision = cross_val_score(rdf_smote, x_smote, y_smote, cv=5, scoring='precision_weighted')\n",
    "print('SMOTE_RDF Precision: ', np.mean(rdfSmote_precision))\n",
    "rdfSmote_f1 = cross_val_score(rdf_smote, x_smote, y_smote, cv=5, scoring='f1_weighted')\n",
    "print('SMOTE_RDF F1: ', np.mean(rdfSmote_f1))\n",
    "rdfSmote_accuracy = cross_val_score(rdf_smote, x_smote, y_smote, cv=5, scoring='accuracy')\n",
    "print('SMOTE_RDF Accuracy: ', np.mean(rdfSmote_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average confusion matrix after every fold Cross-Validation:  \n",
      " [[16.  1.  0.]\n",
      " [ 0. 15.  1.]\n",
      " [ 0.  3. 14.]]\n"
     ]
    }
   ],
   "source": [
    "#SMOTE + Optimized RDFconfusion matrix:\n",
    "\n",
    "avg_conf_matrix(rdf_smote, x_smote, y_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice that the number of total observations in the confusion matrix == 50, this means for every k-fold classification \" ~= 50 samples are held out as validation data\" and the remaining ~= 196 are used as training data.**\n",
    "\n",
    "- This different from the baseline model number of observations which was ~= 28, because we didnt train the base model on **Oversampled data**, but ***this does not affect the performance of the evaluation metric or the others,*** as you can see **SMOTE + OPTIMIZED RANDOM FOREST** has far better performance than the **Baseline Model.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd Approach - Using Cost Sensitive RandomForestClassifier\n",
    "\n",
    "- This is primarily based on the **class_weights of our model.**\n",
    "\n",
    "\n",
    "- Here, We are NOT using the SMOTE data because the class weight - takes into account the imbalance in our data set, This is done by imposing a cost penalty on the minority class. Different methods of adding weight to the minority class will be evaluated in this section.\n",
    "\n",
    "\n",
    "- The class_weight is a dictionary that defines each class label and the weighting to apply in the calculation of group purity for splits in the decision tree when fitting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=RepeatedStratifiedKFold(n_repeats=3, n_splits=5, random_state=1),\n",
       "             error_score=nan,\n",
       "             estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                              class_weight={0: 5, 1: 1, 2: 2},\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              max_samples=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samp...\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators=100, n_jobs=None,\n",
       "                                              oob_score=False,\n",
       "                                              random_state=None, verbose=0,\n",
       "                                              warm_start=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'max_depth': [6, 8, 10, 20],\n",
       "                         'min_samples_leaf': [4, 8, 12],\n",
       "                         'min_samples_split': [5, 10, 15],\n",
       "                         'n_estimators': [300, 400, 500]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first of all we will used the Define the weight for each class and perform GridSearch \n",
    "#Here, higher weight are assigned to the minority classes\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits = 5, n_repeats=3, random_state = 1)\n",
    "\n",
    "rdf = RandomForestClassifier(class_weight = dict({0:5, 1:1, 2:2}))\n",
    "\n",
    "params = {'max_depth': [6, 8, 10, 20], 'min_samples_split': [5, 10, 15], \n",
    "          'min_samples_leaf' : [4, 8, 12], 'n_estimators' : [300, 400, 500],}\n",
    "\n",
    "grid_clf_weight = GridSearchCV(estimator = rdf, param_grid = params, cv=cv)\n",
    "grid_clf_weight.fit(x_trn, y_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 20, 'min_samples_leaf': 12, 'min_samples_split': 10, 'n_estimators': 400}\n"
     ]
    }
   ],
   "source": [
    "#best parameters for the model\n",
    "print(grid_clf_weight.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets get the model\n",
    "rdf_weight = grid_clf_weight.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted_RDF Recall:  0.6\n",
      "Weighted_RDF Precision:  0.5321240426240427\n",
      "Weighted_RDF F1:  0.4709003298289013\n",
      "Weighted_RDF Accuracy:  0.5928571428571429\n"
     ]
    }
   ],
   "source": [
    "#lets evaluate the model\n",
    "\n",
    "rdfweight_recall = cross_val_score(rdf_weight, x_trn, y_trn, cv=5, scoring='recall_weighted')\n",
    "print('Weighted_RDF Recall: ', np.mean(rdfweight_recall))\n",
    "rdfweight_precision = cross_val_score(rdf_weight, x_trn, y_trn, cv=5, scoring='precision_weighted')\n",
    "print('Weighted_RDF Precision: ', np.mean(rdfweight_precision))\n",
    "rdfweight_f1 = cross_val_score(rdf_weight, x_trn, y_trn, cv=5, scoring='f1_weighted')\n",
    "print('Weighted_RDF F1: ', np.mean(rdfweight_f1))\n",
    "rdfweight_accuracy = cross_val_score(rdf_weight, x_trn, y_trn, cv=5, scoring='accuracy')\n",
    "print('Weighted_RDF Accuracy: ', np.mean(rdfweight_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average confusion matrix after every fold Cross-Validation:  \n",
      " [[ 0.  3.  0.]\n",
      " [ 0. 15.  1.]\n",
      " [ 0.  7.  1.]]\n"
     ]
    }
   ],
   "source": [
    "#Cost Sensitive Optimized RDFconfusion matrix:\n",
    "\n",
    "avg_conf_matrix(rdf_weight, x_trn, y_trn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The result of this first Weighted random classifier is similar to the baseline model, this means we need to find another way to optimize this randomclassifier**\n",
    "\n",
    "- In the next section, we will try the **class_weight == 'balanced' argument value** AND **Random Forest With Bootstrap Class Weighting Method** on the RandomForestClassifier using gridsearch.\n",
    "\n",
    "\n",
    "- \"**class_weight\"** argument value of ‘balanced‘ can be provided to automatically use the inverse weighting from the training dataset, giving focus to the minority class.\n",
    "\n",
    "#### Random Forest With Bootstrap Class Weighting Method\n",
    "\n",
    "- Given that each decision tree is constructed from a bootstrap sample (e.g. random selection with replacement), the class distribution in the data sample will be different for each tree.\n",
    "\n",
    "- As such, it might be interesting to change the class weighting based on the class distribution in each bootstrap sample, instead of the entire training dataset.\n",
    "\n",
    "- This can be achieved by setting the class_weight argument to the value ‘balanced_subsample‘\n",
    "- [reference](https://machinelearningmastery.com/bagging-and-random-forest-for-imbalanced-classification/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=RepeatedStratifiedKFold(n_repeats=3, n_splits=5, random_state=1),\n",
       "             error_score=nan,\n",
       "             estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                              class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              max_samples=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split...\n",
       "                                              n_estimators=100, n_jobs=None,\n",
       "                                              oob_score=False,\n",
       "                                              random_state=None, verbose=0,\n",
       "                                              warm_start=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'class_weight': ['balanced', 'balanced_subsample'],\n",
       "                         'max_depth': [6, 8, 10, 20],\n",
       "                         'min_samples_leaf': [4, 8, 12],\n",
       "                         'min_samples_split': [5, 10, 15],\n",
       "                         'n_estimators': [300, 400, 500]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let try the above-described method using GridSearchCV\n",
    "cv = RepeatedStratifiedKFold(n_splits = 5, n_repeats=3, random_state = 1)\n",
    "\n",
    "rdf = RandomForestClassifier()\n",
    "\n",
    "params = {'max_depth': [6, 8, 10, 20], 'min_samples_split': [5, 10, 15], \n",
    "          'min_samples_leaf' : [4, 8, 12], 'n_estimators' : [300, 400, 500],\n",
    "         'class_weight': ['balanced', 'balanced_subsample']}\n",
    "\n",
    "grid_clf_bal = GridSearchCV(estimator = rdf, param_grid = params, cv=cv)\n",
    "grid_clf_bal.fit(x_trn, y_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class_weight': 'balanced', 'max_depth': 6, 'min_samples_leaf': 8, 'min_samples_split': 10, 'n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "#best parameters for the model\n",
    "print(grid_clf_bal.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets get the model\n",
    "rdf_bal = grid_clf_bal.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted_RDF Recall:  0.5857142857142857\n",
      "Weighted_RDF Precision:  0.4014739229024943\n",
      "Weighted_RDF F1:  0.4738522385698466\n",
      "Weighted_RDF Accuracy:  0.6\n"
     ]
    }
   ],
   "source": [
    "#lets evaluate the model\n",
    "\n",
    "rdfbal_recall = cross_val_score(rdf_bal, x_trn, y_trn, cv=5, scoring='recall_weighted')\n",
    "print('Weighted_RDF Recall: ', np.mean(rdfbal_recall))\n",
    "rdfbal_precision = cross_val_score(rdf_bal, x_trn, y_trn, cv=5, scoring='precision_weighted')\n",
    "print('Weighted_RDF Precision: ', np.mean(rdfbal_precision))\n",
    "rdfbal_f1 = cross_val_score(rdf_bal, x_trn, y_trn, cv=5, scoring='f1_weighted')\n",
    "print('Weighted_RDF F1: ', np.mean(rdfbal_f1))\n",
    "rdfbal_accuracy = cross_val_score(rdf_bal, x_trn, y_trn, cv=5, scoring='accuracy')\n",
    "print('Weighted_RDF Accuracy: ', np.mean(rdfbal_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average confusion matrix after every fold Cross-Validation:  \n",
      " [[ 0.  3.  0.]\n",
      " [ 0. 16.  0.]\n",
      " [ 0.  8.  1.]]\n"
     ]
    }
   ],
   "source": [
    "#Cost Sensitive Optimized RDFconfusion matrix:\n",
    "\n",
    "avg_conf_matrix(rdf_bal, x_trn, y_trn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **This performance of the evaluation metrics in the Weighted Random Forest Classifier does not differ much from the Baseline Model this means we are not going to further use it**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3rd Approach - Training the LDA - Linear Discrminant Analysis classifier on the SMOTE data\n",
    "\n",
    "- LDA is a supervised multic-class classifier and a dimensionality reduction technique. It is used  for both classification and as a pre-processing step in Machine Learning.\n",
    "\n",
    "- The only reason we did not use it initially as a dimensionality reduction technique is because LDA is said to be affected by imbalanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 3 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed:    3.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=RepeatedStratifiedKFold(n_repeats=3, n_splits=5, random_state=1),\n",
       "             error_score=nan,\n",
       "             estimator=LinearDiscriminantAnalysis(n_components=None,\n",
       "                                                  priors=None, shrinkage=None,\n",
       "                                                  solver='svd',\n",
       "                                                  store_covariance=False,\n",
       "                                                  tol=0.0001),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'solver': ['svd', 'lsqr', 'eigen']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=4)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets start hyperparameter tuning\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits = 5, n_repeats=3, random_state = 1)\n",
    "\n",
    "#LDA classifier\n",
    "\n",
    "lda = LDA()\n",
    "\n",
    "params = {'solver': ['svd','lsqr','eigen']}\n",
    "\n",
    "grid_lda = GridSearchCV(estimator = lda, param_grid = params, cv=cv, n_jobs=-1, verbose=4)\n",
    "grid_lda.fit(x_smote, y_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'solver': 'svd'}\n"
     ]
    }
   ],
   "source": [
    "#best parameters for the model\n",
    "print(grid_lda.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets get the model\n",
    "lda = grid_lda.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE_LDA Recall:  0.8251428571428571\n",
      "SMOTE_LDA Precision:  0.8398435474258056\n",
      "SMOTE_LDA F1:  0.813885825793885\n",
      "SMOTE_LDA Accuracy:  0.8251428571428571\n"
     ]
    }
   ],
   "source": [
    "#lets evaluate LDA classifier\n",
    "\n",
    "lda_recall = cross_val_score(lda, x_smote, y_smote, cv=5, scoring='recall_weighted')\n",
    "print('SMOTE_LDA Recall: ', np.mean(lda_recall))\n",
    "lda_precision = cross_val_score(lda, x_smote, y_smote, cv=5, scoring='precision_weighted')\n",
    "print('SMOTE_LDA Precision: ', np.mean(lda_precision))\n",
    "lda_f1 = cross_val_score(lda, x_smote, y_smote, cv=5, scoring='f1_weighted')\n",
    "print('SMOTE_LDA F1: ', np.mean(lda_f1))\n",
    "lda_accuracy = cross_val_score(lda, x_smote, y_smote, cv=5, scoring='accuracy')\n",
    "print('SMOTE_LDA Accuracy: ', np.mean(lda_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average confusion matrix after every fold Cross-Validation:  \n",
      " [[16.  0.  0.]\n",
      " [ 2. 12.  2.]\n",
      " [ 1.  1. 15.]]\n"
     ]
    }
   ],
   "source": [
    "#SMOTE + Optimized RDFconfusion matrix:\n",
    "\n",
    "avg_conf_matrix(lda, x_smote, y_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The SMOTE + LDA combination also performs far better than the Baseline Model and the Weighted RandomForestClassifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in Test Data and generate predictions on the data using the SMOTE_RDF model (has the highest evaluation score)\n",
    "\n",
    "- Before we perform predictions on the test data, we must perform one-hot encoder on its categorical columns, then transform the entire data using the scaler method we used earlier and finally transform it also using the PCA we used earlier. Note: We only perform **Transformation on the data not FITTING for both Scaler and PCA**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in test data\n",
    "df_test = pd.read_csv('test_features.csv', names = ['feat_' + str(x) for x in range(1,904)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>feat_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_894</th>\n",
       "      <th>feat_895</th>\n",
       "      <th>feat_896</th>\n",
       "      <th>feat_897</th>\n",
       "      <th>feat_898</th>\n",
       "      <th>feat_899</th>\n",
       "      <th>feat_900</th>\n",
       "      <th>feat_901</th>\n",
       "      <th>feat_902</th>\n",
       "      <th>feat_903</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>left</td>\n",
       "      <td>type_1</td>\n",
       "      <td>blue</td>\n",
       "      <td>-0.0911</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>1.8206</td>\n",
       "      <td>-5.1902</td>\n",
       "      <td>3.0139</td>\n",
       "      <td>-1.4510</td>\n",
       "      <td>-4.9192</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.6555</td>\n",
       "      <td>5.5905</td>\n",
       "      <td>7.7045</td>\n",
       "      <td>4.3253</td>\n",
       "      <td>0.7936</td>\n",
       "      <td>-0.6663</td>\n",
       "      <td>2.3355</td>\n",
       "      <td>0.3455</td>\n",
       "      <td>-2.0936</td>\n",
       "      <td>3.0695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>right</td>\n",
       "      <td>type_3</td>\n",
       "      <td>green</td>\n",
       "      <td>-7.9454</td>\n",
       "      <td>-1.9499</td>\n",
       "      <td>1.1313</td>\n",
       "      <td>-0.2745</td>\n",
       "      <td>3.6763</td>\n",
       "      <td>1.2331</td>\n",
       "      <td>-0.7179</td>\n",
       "      <td>...</td>\n",
       "      <td>3.5414</td>\n",
       "      <td>-0.6960</td>\n",
       "      <td>2.5997</td>\n",
       "      <td>-0.7971</td>\n",
       "      <td>-5.1642</td>\n",
       "      <td>-0.7928</td>\n",
       "      <td>-1.8232</td>\n",
       "      <td>0.6038</td>\n",
       "      <td>-6.7637</td>\n",
       "      <td>-1.8046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>right</td>\n",
       "      <td>type_3</td>\n",
       "      <td>blue</td>\n",
       "      <td>-4.9606</td>\n",
       "      <td>2.6980</td>\n",
       "      <td>0.0592</td>\n",
       "      <td>2.0149</td>\n",
       "      <td>1.7465</td>\n",
       "      <td>4.3341</td>\n",
       "      <td>6.7963</td>\n",
       "      <td>...</td>\n",
       "      <td>1.4680</td>\n",
       "      <td>-5.9941</td>\n",
       "      <td>2.4825</td>\n",
       "      <td>1.0533</td>\n",
       "      <td>0.4715</td>\n",
       "      <td>3.4329</td>\n",
       "      <td>-2.8285</td>\n",
       "      <td>-1.5425</td>\n",
       "      <td>6.9466</td>\n",
       "      <td>-2.8522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>right</td>\n",
       "      <td>type_2</td>\n",
       "      <td>blue</td>\n",
       "      <td>1.9404</td>\n",
       "      <td>-1.1492</td>\n",
       "      <td>-2.7282</td>\n",
       "      <td>0.8090</td>\n",
       "      <td>3.4097</td>\n",
       "      <td>2.0182</td>\n",
       "      <td>-1.1578</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.5082</td>\n",
       "      <td>-0.2061</td>\n",
       "      <td>4.1618</td>\n",
       "      <td>-2.7309</td>\n",
       "      <td>4.6747</td>\n",
       "      <td>-0.3183</td>\n",
       "      <td>1.3629</td>\n",
       "      <td>1.3480</td>\n",
       "      <td>5.3539</td>\n",
       "      <td>-4.7206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>left</td>\n",
       "      <td>type_2</td>\n",
       "      <td>blue</td>\n",
       "      <td>-6.6614</td>\n",
       "      <td>-2.3878</td>\n",
       "      <td>-2.0364</td>\n",
       "      <td>-4.9774</td>\n",
       "      <td>3.7972</td>\n",
       "      <td>-1.9987</td>\n",
       "      <td>-0.4928</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.6871</td>\n",
       "      <td>0.3611</td>\n",
       "      <td>-3.2261</td>\n",
       "      <td>1.8367</td>\n",
       "      <td>-0.5666</td>\n",
       "      <td>-3.0393</td>\n",
       "      <td>-2.2726</td>\n",
       "      <td>3.9492</td>\n",
       "      <td>-0.9959</td>\n",
       "      <td>4.9632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 903 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  feat_1  feat_2 feat_3  feat_4  feat_5  feat_6  feat_7  feat_8  feat_9  \\\n",
       "0   left  type_1   blue -0.0911  0.5625  1.8206 -5.1902  3.0139 -1.4510   \n",
       "1  right  type_3  green -7.9454 -1.9499  1.1313 -0.2745  3.6763  1.2331   \n",
       "2  right  type_3   blue -4.9606  2.6980  0.0592  2.0149  1.7465  4.3341   \n",
       "3  right  type_2   blue  1.9404 -1.1492 -2.7282  0.8090  3.4097  2.0182   \n",
       "4   left  type_2   blue -6.6614 -2.3878 -2.0364 -4.9774  3.7972 -1.9987   \n",
       "\n",
       "   feat_10  ...  feat_894  feat_895  feat_896  feat_897  feat_898  feat_899  \\\n",
       "0  -4.9192  ...   -2.6555    5.5905    7.7045    4.3253    0.7936   -0.6663   \n",
       "1  -0.7179  ...    3.5414   -0.6960    2.5997   -0.7971   -5.1642   -0.7928   \n",
       "2   6.7963  ...    1.4680   -5.9941    2.4825    1.0533    0.4715    3.4329   \n",
       "3  -1.1578  ...   -0.5082   -0.2061    4.1618   -2.7309    4.6747   -0.3183   \n",
       "4  -0.4928  ...   -2.6871    0.3611   -3.2261    1.8367   -0.5666   -3.0393   \n",
       "\n",
       "   feat_900  feat_901  feat_902  feat_903  \n",
       "0    2.3355    0.3455   -2.0936    3.0695  \n",
       "1   -1.8232    0.6038   -6.7637   -1.8046  \n",
       "2   -2.8285   -1.5425    6.9466   -2.8522  \n",
       "3    1.3629    1.3480    5.3539   -4.7206  \n",
       "4   -2.2726    3.9492   -0.9959    4.9632  \n",
       "\n",
       "[5 rows x 903 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(560, 903)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_trans = make_column_transformer((ohe, ['feat_1', 'feat_2', 'feat_3']), remainder = 'passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformed the categorical columns in the test data to binary columns\n",
    "df_trans_tst = pd.DataFrame(col_trans.fit_transform(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>898</th>\n",
       "      <th>899</th>\n",
       "      <th>900</th>\n",
       "      <th>901</th>\n",
       "      <th>902</th>\n",
       "      <th>903</th>\n",
       "      <th>904</th>\n",
       "      <th>905</th>\n",
       "      <th>906</th>\n",
       "      <th>907</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0911</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.6555</td>\n",
       "      <td>5.5905</td>\n",
       "      <td>7.7045</td>\n",
       "      <td>4.3253</td>\n",
       "      <td>0.7936</td>\n",
       "      <td>-0.6663</td>\n",
       "      <td>2.3355</td>\n",
       "      <td>0.3455</td>\n",
       "      <td>-2.0936</td>\n",
       "      <td>3.0695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.9454</td>\n",
       "      <td>-1.9499</td>\n",
       "      <td>...</td>\n",
       "      <td>3.5414</td>\n",
       "      <td>-0.6960</td>\n",
       "      <td>2.5997</td>\n",
       "      <td>-0.7971</td>\n",
       "      <td>-5.1642</td>\n",
       "      <td>-0.7928</td>\n",
       "      <td>-1.8232</td>\n",
       "      <td>0.6038</td>\n",
       "      <td>-6.7637</td>\n",
       "      <td>-1.8046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.9606</td>\n",
       "      <td>2.6980</td>\n",
       "      <td>...</td>\n",
       "      <td>1.4680</td>\n",
       "      <td>-5.9941</td>\n",
       "      <td>2.4825</td>\n",
       "      <td>1.0533</td>\n",
       "      <td>0.4715</td>\n",
       "      <td>3.4329</td>\n",
       "      <td>-2.8285</td>\n",
       "      <td>-1.5425</td>\n",
       "      <td>6.9466</td>\n",
       "      <td>-2.8522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.9404</td>\n",
       "      <td>-1.1492</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.5082</td>\n",
       "      <td>-0.2061</td>\n",
       "      <td>4.1618</td>\n",
       "      <td>-2.7309</td>\n",
       "      <td>4.6747</td>\n",
       "      <td>-0.3183</td>\n",
       "      <td>1.3629</td>\n",
       "      <td>1.3480</td>\n",
       "      <td>5.3539</td>\n",
       "      <td>-4.7206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.6614</td>\n",
       "      <td>-2.3878</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.6871</td>\n",
       "      <td>0.3611</td>\n",
       "      <td>-3.2261</td>\n",
       "      <td>1.8367</td>\n",
       "      <td>-0.5666</td>\n",
       "      <td>-3.0393</td>\n",
       "      <td>-2.2726</td>\n",
       "      <td>3.9492</td>\n",
       "      <td>-0.9959</td>\n",
       "      <td>4.9632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 908 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7       8       9    ...     898  \\\n",
       "0  1.0  0.0  1.0  0.0  0.0  1.0  0.0  0.0 -0.0911  0.5625  ... -2.6555   \n",
       "1  0.0  1.0  0.0  0.0  1.0  0.0  1.0  0.0 -7.9454 -1.9499  ...  3.5414   \n",
       "2  0.0  1.0  0.0  0.0  1.0  1.0  0.0  0.0 -4.9606  2.6980  ...  1.4680   \n",
       "3  0.0  1.0  0.0  1.0  0.0  1.0  0.0  0.0  1.9404 -1.1492  ... -0.5082   \n",
       "4  1.0  0.0  0.0  1.0  0.0  1.0  0.0  0.0 -6.6614 -2.3878  ... -2.6871   \n",
       "\n",
       "      899     900     901     902     903     904     905     906     907  \n",
       "0  5.5905  7.7045  4.3253  0.7936 -0.6663  2.3355  0.3455 -2.0936  3.0695  \n",
       "1 -0.6960  2.5997 -0.7971 -5.1642 -0.7928 -1.8232  0.6038 -6.7637 -1.8046  \n",
       "2 -5.9941  2.4825  1.0533  0.4715  3.4329 -2.8285 -1.5425  6.9466 -2.8522  \n",
       "3 -0.2061  4.1618 -2.7309  4.6747 -0.3183  1.3629  1.3480  5.3539 -4.7206  \n",
       "4  0.3611 -3.2261  1.8367 -0.5666 -3.0393 -2.2726  3.9492 -0.9959  4.9632  \n",
       "\n",
       "[5 rows x 908 columns]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trans_tst.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just transform\n",
    "tst_scale = scaler.transform(df_trans_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_test = pca.transform(tst_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = pd.DataFrame(pc_test, columns = ['pc_' + str(x) for x in range(1,101)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let perform prediction on the test data\n",
    "\n",
    "test_pred = rdf_smote.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tst_pred = pd.DataFrame(test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the predicted test labels into a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tst_pred.to_csv('test_labels.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "- This task was focused on performing multiclass classification on **imbalanced multiclass dataset with large number of columns**.\n",
    "\n",
    "\n",
    "- Data preprocessing techniques such as One-hot Encoder and Standardization were utilized for preparing the data for further analysis.\n",
    "\n",
    "\n",
    "- Dimensionality Reduction was done on the training dataset to be able to reduce and eliminate redundant features in the data and further keep only features or principal components that explains the variance in the data.\n",
    "\n",
    "\n",
    "- Three approach were utilized to combat both the imbalance and large number of column problems in our data.\n",
    "\n",
    "\n",
    "- RandomForest Classifier was the major machine learning model utilized in this classification as it is an ensemble machine learning algorithm that follows the bagging technique, that is said to perform more better than other bagging and boosting techniques when optimized appropriately.\n",
    "\n",
    "\n",
    "- Finally, Borderline SMOTE + Optimized Random Forest Classifier were the best approach for this multiclass classification task.\n",
    "\n",
    "\n",
    "### Outlook\n",
    "\n",
    "- ***More advanced dimensionality reduction, oversampling and machine learning technique such as t-SNE, ADASYN, Artificial Neural Network Autoencoders,  can be further tried on this multiclass classification problem to get better evaluation scores.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
